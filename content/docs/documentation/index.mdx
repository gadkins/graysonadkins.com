---
title: Tokenization
description: Understand the role of tokens in natural language processing.
---

Tokenization is the process of splitting a piece of text, such as a sentence or a document, into these smaller units, which are called tokens.

## Key functions

Tokenization is a crucial step in preparing text data for various NLP tasks like machine translation, sentiment analysis, text classification, and text generation.   

Transformer-based language models, like GPT-4, rely heavily on tokenization to process input text into a format that the model can understand and work with efficiently. 
The tokens generated by the tokenizer are often converted into embeddings or numerical representations that can be fed into the LLM for further processing and analysis.  

A tokenizer has several key functions:  

1. **Text Segmentation**. Tokenization divides continuous text into individual units, which can be words or subword pieces. This segmentation is essential for various NLP tasks, as it provides a structured representation of the input text.
2. **Normalization**. Tokenizers often perform text normalization tasks, such as converting text to lowercase, removing punctuation, and handling special characters, to ensure consistency in tokenization.
3. **Subword Tokenization**. Some tokenizers use subword tokenization techniques, like Byte-Pair Encoding (BPE) or WordPiece, to split words into smaller meaningful units. Subword tokenization is particularly useful for handling languages with complex word structures and for efficiently representing a wide vocabulary.
4. **Handling Special Cases**. Tokenizers handle special cases like contractions ("can't" becomes "can" and "n't") and possessives ("cat's" becomes "cat" and "'s") to ensure that text is tokenized sensibly.
5. **Vocabulary Management**. Tokenizers often manage a vocabulary or dictionary of known tokens. When a token is encountered that is not in the vocabulary, it may be handled using special tokens (e.g., [UNK] for "unknown") or subword tokenization.
6. **Preprocessing**. Tokenizers are an integral part of preprocessing text data for NLP tasks, making it suitable for training and inference with language models.

## Types of tokenization

Take the following sentence as an example input sequence:  

> **"The quick brown fox jumps over the lazy dog."**  

The sentence can be tokenized in a few different ways:  

- **Word Tokenization**. The sentence is split into individual words:  
`["The", "quick", "brown", "fox", "jumps", "over", "the", "lazy", "dog", "."]`  
- **Character Tokenization**. The sentence is split into individual characters:  
`["T", "h", "e", " ", "q", "u", "i", "c", "k", " ", "b", "r", "o", ..."."]`  
- **Subword Tokenization**. The sentence is split into subword units:  
`["The", "quic", "k", "brown", "fox", "jump", "s", "over", "the", "lazy", "dog", "."]`  

Subword tokenization is particularly useful for handling languages with complex word structures and for efficiently 
representing a wide vocabulary, as it interpolates between word and character tokenization.  

## Tokenization strategies  

Tokenization can be performed using a simple a set of rules and heuristics or more complex algorithms. In either case, the general process relies 
on patterns and delimiters to identify and split text into tokens. Here are some common tokenization approaches:  

- **Rule-based Tokenization**. redefined rules are applied to split text based on specific patterns 
and delimiters. For example, rules may specify that a space character or specific punctuation marks (e.g., periods, 
commas, question marks) should be used as token boundaries. These rules are often implemented using regular expressions 
or simple string manipulation.  
- **Dictionary-based Tokenization**. This approach uses a predefined dictionary or vocabulary to identify valid words 
in the text. Words that match entries in the dictionary are considered tokens. This approach is often used in 
languages with complex word structures or in domain-specific applications.  
- **Statistical Tokenization**. This approach uses statistical models to identify and split text into tokens. For 
example, a statistical tokenizer may use a language model to predict the likelihood of a word boundary at each 
position in the text.  
- **Maximum Matching Algorithm**. This algorithm is commonly used in Chinese word segmentation, where words are not 
separated by spaces. It works by starting at the beginning of a sentence and attempting to find the longest valid word 
at each step based on a predefined lexicon. The process continues until the entire text is tokenized.  

## Subword tokenization algorithms  

- **Byte-Pair Encoding (BPE)**. Orginally developed for data compression, BPE works by iteratively merging the most frequent character or byte pairs in a given corpus to create a vocabulary of subword units. BPE is employed in 
GPT-2, RoBERTa, and other LLMs. See the [tokenziation course on Hugging Face](https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt) for an detailed guide on BPE.   
- **Wordpiece**. WordPiece is a subword tokenization algorithm developed by Google and used in [BERT](https://arxiv.org/abs/1810.04805). It is similar to BPE but uses a different merging strategy. [Read more →](https://huggingface.co/docs/transformers/tokenizer_summary#wordpiece)  
- **Unigram**. This algorithm starts with a large base vocabulary—such as all words and common substrings—then iteratively prunes the vocab based on the log likelihood of a loss function if the symbol were removed from the vocabulary.  
- **SentencePiece**. To overcome the limitation that Unigram assumes words are separated by spaces (which is not the case for every language), SentencePiece uses language-specific pre-tokenization rules to segment text into sentences. It then applies the Unigram algorithm to the sentences.  

## Implementations  

Here are some popular libraries and tools for the tokenization strategies and algorithms mentioned above:  

- [`tokenizers`](https://huggingface.co/docs/tokenizers/index) (Hugging Face)  
- [`torchtext`](https://pytorch.org/text/stable/index.html) (PyTorch)  
- [`tf.keras.layers.TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) (TensorFlow Keras)  
- [NLTK](https://www.nltk.org)  
- [spaCy](https://spacy.io)  
