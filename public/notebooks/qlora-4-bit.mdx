---
title: Parameter-Efficient Fine-Turning of Llama 2 with QLoRA
description: 4-bit quantization of Llama 2 with Low-Rank Adaptation
date: 2023-12-20
---

This notebook demonstrates basic parameter-efficient fine-tuning (PEFT) of Llama 2 using 4-bit quantized LoRA (QLoRA). It leverages the transformers and PEFT libraries from Hugging Face for quantization, LoRA, and fine-tuning.

This notebook is based on [this blog by HF](https://huggingface.co/blog/4bit-transformers-bitsandbytes) and closely follows the outline of [this notebook](https://colab.research.google.com/drive/1uMSS1o_8YOPyG1X_4k6ENEE3kJfBGGhH?usp=sharing#scrollTo=XIyP_0r6zuVc) from Trelis Research.

## Setup

