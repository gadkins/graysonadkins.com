{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDdy-k2BVDAo",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "# **Fine-tuning Open-Source LLMs for Function Calling**\n",
        "\n",
        "This notebook demonstrates fine-tuning of an open-source model ([Mistral 7B](https://mistral.ai/news/announcing-mistral-7b/)). It leverages the `transformers` and `PEFT` libraries from Hugging Face for quantization, LoRA, and training, and a custom-built data set for function calling.\n",
        "\n",
        "This notebook builds on the [basic fine-tuning](/notebooks/fine-tuning/basic) example by introducing the following innovations:  \n",
        "- A prompt loss mask to focus the model's attention and encourage structured responses  \n",
        "- A stop sequence after responses to encourage conciseness  \n",
        "- A small, but high-quality function-calling data set to fine-tune the model for responding with functions and query parametes  \n",
        "\n",
        "**Notes**  \n",
        "- The example data set used in this notebook is for function calling but these techniques work for any Q&A data set.  \n",
        "- The system prompts have been omitted, but you can add them back if you wish to fine-tune for a certain system message.\n",
        "- While you can run this notebook on an NVIDIA T4 GPU (free on Google Colab), I recommend using an A6000 or A100 to get better results. These larger machines are available in Google Colab Pro or at RunPod, Lambda Labs, et al.)\n",
        "\n",
        "**Recommended Reading**\n",
        "- [Tokenization](/docs/handbook/tokenization)\n",
        "- [Low-Rank Adaption (LoRA)](/docs/handbook/lora)\n",
        "\n",
        "**Attribution**\n",
        "- Some functions in this notebook were adapted from [Trelis](https://trelis.com) examples with modifications.\n",
        "- A related, but simpler training notebook by Hugging Face is available [here](https://colab.research.google.com/drive/1VoYNfYDKcKRQRor98Zbf2-9VQTtGJ24k?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why should you read this notebook?\n",
        "\n",
        "You want to learn how to:\n",
        "- Fine-tune an open-source model for structured and concise\n",
        "responses\n",
        "- Fine-tune using just a single GPU  \n",
        "- Learn how to use prompt loss-masks for controlling model attention"
      ],
      "metadata": {
        "id": "hDrd5igmp6ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Concepts"
      ],
      "metadata": {
        "id": "ozDCFkHravyM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4puWMpDVGQ3",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "Typically, a model is graded on its prediction of the next token in both the question and answer. However, our primary goal is for the model to give thoughtful attention to the question, while its performance should be graded based soley on how it predicts the answer; this is achieved by attention and loss masks, respectively.  \n",
        "\n",
        "## Attention mask\n",
        "Attention is a mechanism used during training to instruct the model on what parts of the input text (e.g., a question or a context) it should pay attention to. It helps the model focus on the relevant information and ignore irrelevant portions of the input. An attention mask is simply a sequence of 1s and 0s that is multiplied by the input sequence IDs—resulting in a new input sequence where irrelevant tokens are zeroed out (i.e. masked).\n",
        "\n",
        "```\n",
        "{'input_ids': tensor([[9204, 18, 3763, 456, 222, 13563, 22580, 584]]),\n",
        " 'attention_mask': tensor([[1, 1, 1, 0, 1, 1, 0, 1]])}\n",
        "```\n",
        "\n",
        "```\n",
        "{'result': tensor([[9204, 18, 3763, 0, 222, 13563, 0, 584]])}\n",
        "```\n",
        "\n",
        "As an example, we usually want to make sure that `PAD` tokens are masked.\n",
        "\n",
        "## Loss mask\n",
        "\n",
        "A loss mask is used to calculate the loss or error during training. It specifies which parts of the model's output should be considered when computing the loss. When training a model, we take the losses and multiply them by the loss mask.\n",
        "\n",
        "To improve model performance, in this notebook we mask the losses associated with prompt to ensure the model focuses on answering the question, not predicting the next sequence of tokens in the question.\n",
        "\n",
        "\n",
        "## Stop sequence\n",
        "Have you every noticed how verbose some models are? By fine-tuning with stop sequence, such as `USER:`, we can teach the model to be more concise:\n",
        "\n",
        "```\n",
        "{\n",
        "  prompt: \"Where is the stock price of Apple?\\n\\nBOT:\",\n",
        "  completion: \"Apple stock price is $188.04.\\n\\nUSER: \",\n",
        "},\n",
        "...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mistral 7B Instruct\n",
        "\n",
        "Mistral 7B Instruct is an instruction fine-tuned version of Mistral 7B available on [Hugging Face](https://huggingface.co/mistralai/Mistral-7B-v0.1).\n",
        "\n",
        "Per the HF model card:  \n",
        "\n",
        "> ## Instruction format\n",
        "The template used to build a prompt for the Instruct model is defined as follows:\n",
        ">\n",
        "> ```\n",
        "<s> [INST] Instruction [/INST] Model answer</s> [INST] Follow-up instruction [/INST]\n",
        "```\n",
        ">\n",
        "> ## Model architecture\n",
        ">\n",
        ">This instruction model is based on Mistral-7B-v0.1, a transformer model with the following architecture choices:  \n",
        ">\n",
        ">- Grouped-Query Attention  \n",
        "- Sliding-Window Attention  \n",
        "- Byte-fallback BPE tokenizer  "
      ],
      "metadata": {
        "id": "1Dvq6fIOCdA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Setup\n",
        "- You can run QLoRa training on a free Google Colab Notebook for 7B models.\n",
        "- To configure a GPU on Google Colab, navigate to **Connect to a new runtime** and select **T4 High-RAM**.  \n",
        "- In the code below, be sure to comment out flash attention when loading the model since flash is only supported on newer Ampere GPUs (A6000, A100, H100, etc.) and not in T4s.\n",
        "- (Optional) Uncomment the code to mount Google Drive to download the model to your Google Drive. This will reduce total start time.\n",
        "- If you don't already have one, create a [Hugging Face account](https://huggingface.co) and [create an Access Token](https://huggingface.co/settings/tokens) called \"Notebooks\" or similar with `write` permissions."
      ],
      "metadata": {
        "id": "7n__Z2Iip1bP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print GPU info\n",
        "# gpu_info = !nvidia-smi\n",
        "# gpu_info = '\\n'.join(gpu_info)\n",
        "# if gpu_info.find('failed') >= 0:\n",
        "#   print('Not connected to a GPU')\n",
        "# else:\n",
        "#   print(gpu_info)"
      ],
      "metadata": {
        "id": "vATtCUVeHoD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Print VRAM\n",
        "# from psutil import virtual_memory\n",
        "# ram_gb = virtual_memory().total / 1e9\n",
        "# print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "# if ram_gb < 20:\n",
        "#   print('Not using a high-RAM runtime')\n",
        "# else:\n",
        "#   print('You are using a high-RAM runtime!')"
      ],
      "metadata": {
        "id": "MQNcAOIDMFJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzGa3YSCgl4C"
      },
      "source": [
        "# Install"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182,
          "referenced_widgets": [
            "2a33cd392a3d4488b1119724c365290f",
            "d1c91eb027384c8590623822b941e3e4",
            "14418448f03947e88c3a9bfbebf0183f",
            "73b57743e6f448e997fd8645ee3677cc",
            "717c00d598834223b5e99aa31c0d05bc",
            "e6ddf381454042459e25a48fc55a3139",
            "f9ebdf3ff98945fd937e9af65545c851",
            "50037c8610204d7092fef1451285b278",
            "265c9335183445faa053479fbdad35f9",
            "e024fc33a0f6465195428c0005cc33b0",
            "19532d920030411c9782c80559f01dee",
            "d4eb5460630b458abc3fe217d92fb048",
            "419fa56b75314272b563bfa4fa90b193",
            "10d87a5db7af4fb8a7399ae0d6f23313",
            "ca3f74d8f25949e79fc0260caeb7a0d9",
            "a98a805560c846aea6ae7c8c7b8b5c6b",
            "486e9e36bab34844a161857c9b3df4a3",
            "e1f8ca72aed148eaad33cdc76d7c6227",
            "5e8fb36268e9459a9995966cf0ba6ce5",
            "406d3537027a448c85cb60bf74fffd8e",
            "23a4fea617a04e68ba140fc8c6907992",
            "b162838c1003480f847c8516553a2276",
            "3cbe716460d84038980f075c181c1041",
            "a66f7a4a5dda42338d409fbe9a2dd866",
            "14db3902da914a3d8f27265db51f5627",
            "7d32783beed84eeca574322a07f49dd2",
            "2b7ada8a16394233b13bf2b57a7646f7",
            "7e62914d34214fc0bb540a25cc64e5fe",
            "add3a25c14924401be1713d65f97a372",
            "787844df64a64d19aad10b2e686eed73",
            "f9b18000a67744edaa4a985d4ea38b75",
            "31452bbf02dd418d922949b83e4601ee"
          ]
        },
        "id": "OUXX9i0g-vLs",
        "outputId": "c6c1474c-d994-4c86-96de-eab0ad14c73b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a33cd392a3d4488b1119724c365290f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Authenticate to Hugging Face to pull and push models\n",
        "!pip install huggingface_hub -q\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUkOcBbLz9Qv",
        "outputId": "55293369-6a15-44e1-9d9a-57b5ad1444d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgadkins\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# (Optional) Configure Weights & Biases (wandb) to track training runs\n",
        "!pip install wandb -q -U\n",
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "egglIzSXmewE"
      },
      "outputs": [],
      "source": [
        "# base_model = \"./Mistral-7B-Instruct-v0.1-function-calling-v2\"\n",
        "# base_model = \"meta-llama/Llama-2-7b-hf\"\n",
        "# base_model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# base_model = \"meta-llama/Llama-2-13b-chat-hf\"\n",
        "# base_model = \"codellama/CodeLlama-34b-Instruct-hf\"\n",
        "# base_model = \"meta-llama/Llama-2-70b-chat-hf\"\n",
        "base_model = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "# base_model = \"deepseek-ai/deepseek-coder-1.3b-instruct\"\n",
        "# base_model = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
        "# base_model = \"deepseek-ai/deepseek-coder-33b-instruct\"\n",
        "# base_model = \"larryvrh/Yi-34B-200K-Llamafied\"\n",
        "# base_model = \"./Yi-34B-200K-Llamafied-chat-SFT\"\n",
        "# base_model = \"openchat/openchat_3.5\"\n",
        "# base_model = \"SUSTech/SUS-Chat-34B\"\n",
        "# base_model = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
        "# base_model = \"microsoft/phi-2\"\n",
        "\n",
        "cache_dir = '' # Initialise the cache_dir to null.\n",
        "# (Optionally, you can set Google Drive as the cache_dir below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO9O3ecDz9Qx",
        "outputId": "9bc5198d-3048-46ca-fa0c-bd7cc41b8ede"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m55.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.9/150.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m31.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# stable versions\n",
        "\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install -U -q transformers\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate\n",
        "!pip install -q datasets\n",
        "!pip install -q -U scipy\n",
        "!pip install -q -U trl\n",
        "!pip install -U flash-attn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "v_r4SL_KmHGs"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline, AutoConfig\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8c6_o_sFz9Qy"
      },
      "source": [
        "## If using Google Colab + Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oAzlPhWXVKXo",
        "outputId": "b0fdedfb-6591-4929-9cb1-404d5b0e3aed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "MH8ZqAEcVLTH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "cache_dir = \"/content/drive/My Drive/huggingface_cache\"\n",
        "os.makedirs(cache_dir, exist_ok=True) # Ensure the directory exists\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "7wgbsa4U2TBk"
      },
      "outputs": [],
      "source": [
        "# https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alLhGWu9Ngvg"
      },
      "source": [
        "# Load model\n",
        "\n",
        "**Note about quantization:**  \n",
        "\n",
        "In this section, we have the option to load a quantized version of the model (see the [QLoRA notebook](/notebooks/fine-tuning/qlora) for quantization details) to reduce the computation requirements such that it will fit on a free T4 GPU in Google Colab. If cost is most important to you, then I recommend this option—just uncomment the `quantization_config` option below.\n",
        "\n",
        "However, I've observed slightly better performance in function-calling fine-tunes when using models at full precision. Note that if you use full precision, you'll need a larger GPU such as an A100. If you're using Google Colab, you'll need to upgrade to Pro or use another service like RunPod or Lambda Labs (which are a bit cheaper)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "d5ebe4ae3c244e10a750a55ec61f1d5e",
            "7accd84bf90e4350bb2b79feda2f33e5",
            "e29e59c455874c9797cad3949a20d2bc",
            "749f1e2dc09d41ce8f65194f60f84317",
            "ce920fc79c23463ab8e8c97932568410",
            "901e4460ff08446f8e1533a6d36e8255",
            "bea3864b7bd64301acfd11589234770b",
            "db773f747f1f4efb8e6604123a8ec887",
            "ff665f9b2a384ca8b4911eb793b1dc41",
            "953c4c0e58194031936748493e5d9cdc",
            "ca7968a46a4a4560bed4aa09425b9a09"
          ]
        },
        "id": "tz4eBuxZHi7F",
        "outputId": "eebc1385-8b1d-4da2-ac42-96b10879f8f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5ebe4ae3c244e10a750a55ec61f1d5e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# QLoRA config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "# Instantiate model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    # quantization_config=bnb_config, # Uncomment to use quantized version\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        "    #attn_implementation=\"flash_attention_2\", # Supported in Ampere GPUs or newer\n",
        "    cache_dir=cache_dir\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emcg4xfjxvaZ"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DENt9NWsz9Q0"
      },
      "outputs": [],
      "source": [
        "# # Required for certain tokenizers like Yi\n",
        "# !pip install sentencepiece -q -U"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "SEitzH351wCR"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(base_model, cache_dir=cache_dir, trust_remote_code=True)\n",
        "# tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\", cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LykWtjrgH4kq",
        "outputId": "844ec081-c625-42ef-b53f-f79e823ade35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EOS token: </s>\n",
            "EOS token id: 2\n"
          ]
        }
      ],
      "source": [
        "print(\"EOS token:\", tokenizer.eos_token)\n",
        "print(\"EOS token id:\", tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzw1jQdD1ZBR",
        "outputId": "6d44d7b3-a3dd-427e-e43a-7335784d423a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pad token:  None\n",
            "Pad token ID:  None\n"
          ]
        }
      ],
      "source": [
        "# If pad token is None, we'll need to set one in the next section\n",
        "print(\"Pad token: \", tokenizer.pad_token)\n",
        "print(\"Pad token ID: \", tokenizer.pad_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maQX2nVfZxL_",
        "outputId": "534c322b-761d-47b3-925f-7c5504d0a8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaTokenizerFast(name_or_path='mistralai/Mistral-7B-Instruct-v0.1', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Padding to the right (i.e. after) the prompt and response has better results\n",
        "tokenizer.padding_side='right'\n",
        "print(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGdd8TIh1zkg"
      },
      "source": [
        "## Set pad token if none exists\n",
        "Some models already have a pad token set. You can see whether they do or don't from the tokenizer print statement above. **If that's the case, then you don't need to do anything further.**  \n",
        "\n",
        "If no pad token exists, then you have three options:  \n",
        "\n",
        "**Options**  \n",
        "\n",
        "1. Use an existing token in the vocab as the pad token, instead of introducing a new one. This is to avoid having to create a whole new instance of the tokenizer with a new pad token. For this option, we use the existing `<unk>` token (i.e. \"unknown\") to pad—note that this assumes the `<unk>` token exists in the vocab.  \n",
        "2. The next option is to use the EOS token.  \n",
        "3. The last option is to add a pad token. This expands the size of the model embeddings so that's it's no longer a factor of 16, which can slow down inference. So this is the last option.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "4gKbgeEQz9Q1",
        "outputId": "866b1064-e3fc-4638-899b-601e46498b31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found '<unk>' token in tokenizer. Using '<unk>' for pad.\n"
          ]
        }
      ],
      "source": [
        "## (Recommended) OPTION 1\n",
        "# If <unk> is in the tokenizer, set the pad token to <unk>\n",
        "# Else, set pad token to EOS token\n",
        "if '<unk>' in tokenizer.get_vocab():\n",
        "    print('Found \\'<unk>\\' token in tokenizer. Using \\'<unk>\\' for pad.')\n",
        "    # Set the pad token\n",
        "    tokenizer.pad_token = '<unk>'\n",
        "else:\n",
        "    print(f'Using EOS token, \\'{tokenizer.eos_token}\\', for padding')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "## OPTION 2\n",
        "# # Check if the pad token is already in the tokenizer vocabulary\n",
        "# if '<pad>' not in tokenizer.get_vocab():\n",
        "#     print('pad token not in the tokenizer')\n",
        "\n",
        "#     # Add the pad token\n",
        "#     tokenizer.add_tokens(['<pad>'])\n",
        "\n",
        "# # Set the pad token\n",
        "# tokenizer.pad_token = '<pad>'\n",
        "\n",
        "# # Resize token embeddings\n",
        "# model.resize_token_embeddings(len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Y4htqB2zz9Q1",
        "outputId": "f008ff54-c9f4-4cd9-dc12-39832d37e17a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer pad token ID: 0\n",
            "Model pad token ID: 0\n",
            "Model config pad token ID: 0\n",
            "Number of tokens now in tokenizer: 32000\n"
          ]
        }
      ],
      "source": [
        "# Update pad token id in model and its config\n",
        "model.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "# Check if they are equal\n",
        "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID \\\n",
        "does not match the tokenizer's pad token ID!\"\n",
        "\n",
        "# Print the pad token ids\n",
        "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
        "print('Model pad token ID:', model.pad_token_id)\n",
        "print('Model config pad token ID:', model.config.pad_token_id)\n",
        "print('Number of tokens now in tokenizer:', len(tokenizer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uz7UT6cAz9Q1",
        "outputId": "e7a64d8b-01ce-4eff-b120-b0f4c38ccca8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralConfig {\n",
            "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.37.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print model configuration\n",
        "print(model.config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kByEN730YOh7",
        "outputId": "6c2420ea-027b-4192-df7d-1bf0a3cf4a0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beginning of the sequence: Caio! (BOS token: <s>, id: 1)\n",
            "End of the sequence: Caio! (EOS token: </s>, id: 2)\n",
            "Tokens in the string: 2\n",
            "Token IDs: {'input_ids': tensor([[    1, 11013,   691, 28808]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
            "Decoded string: <s> Caio!\n",
            "Attention mask: tensor([[1, 1, 1, 1]])\n"
          ]
        }
      ],
      "source": [
        "# Sample string\n",
        "# sample_string = ['hello [/INST]', 'my good friend</s>']\n",
        "sample_string = ['Caio!']\n",
        "\n",
        "# Tokenize the stringified JSON object\n",
        "encoded_sample = tokenizer(sample_string, truncation=True, padding=True, max_length=1024, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "BOS_token_id = tokenizer.bos_token_id\n",
        "EOS_token_id = tokenizer.eos_token_id\n",
        "BOS_token = tokenizer.decode([BOS_token_id])\n",
        "EOS_token = tokenizer.decode([EOS_token_id])\n",
        "\n",
        "print(f\"Beginning of the sequence: {sample_string[0]} (BOS token: {BOS_token}, id: {BOS_token_id})\")\n",
        "print(f\"End of the sequence: {sample_string[-1]} (EOS token: {EOS_token}, id: {EOS_token_id})\")\n",
        "\n",
        "token_count = len(encoded_sample)\n",
        "\n",
        "print(f\"Tokens in the string: {token_count}\")\n",
        "print(f\"Token IDs: {encoded_sample}\")\n",
        "\n",
        "# Decode the input_ids\n",
        "decoded_sample = tokenizer.decode(encoded_sample['input_ids'][0], skip_special_tokens=False)\n",
        "\n",
        "# Print the decoded string\n",
        "print(f\"Decoded string: {decoded_sample}\")\n",
        "\n",
        "# Print the attention mask\n",
        "print(f\"Attention mask: {encoded_sample['attention_mask']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSuBrec41b0-"
      },
      "source": [
        "## Set up LoRa"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "DWYusaRvperY"
      },
      "outputs": [],
      "source": [
        "# # If loading with adapters\n",
        "# # Note: Instead, it's often faster to download base model then add adapters\n",
        "# from peft import PeftModel\n",
        "\n",
        "# # adapter_model = f'{base_model}' + '-function-calling-adapters' # replace\n",
        "\n",
        "# # Load peft model with adapters\n",
        "# model = PeftModel.from_pretrained(\n",
        "#     model,\n",
        "#     adapter_model,\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Hd22_v_mkbZy"
      },
      "outputs": [],
      "source": [
        "# To reduce VRAM usage (supported by most models)\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# If using quantized model\n",
        "# from peft import prepare_model_for_kbit_training\n",
        "# model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "5EBGtk1On8tc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d8fe644-ec8d-4002-bb04-f1d8e62b5ed7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "odict_keys(['model.embed_tokens.weight', 'model.layers.0.self_attn.q_proj.weight', 'model.layers.0.self_attn.k_proj.weight', 'model.layers.0.self_attn.v_proj.weight', 'model.layers.0.self_attn.o_proj.weight', 'model.layers.0.mlp.gate_proj.weight', 'model.layers.0.mlp.up_proj.weight', 'model.layers.0.mlp.down_proj.weight', 'model.layers.0.input_layernorm.weight', 'model.layers.0.post_attention_layernorm.weight', 'model.layers.1.self_attn.q_proj.weight', 'model.layers.1.self_attn.k_proj.weight', 'model.layers.1.self_attn.v_proj.weight', 'model.layers.1.self_attn.o_proj.weight', 'model.layers.1.mlp.gate_proj.weight', 'model.layers.1.mlp.up_proj.weight', 'model.layers.1.mlp.down_proj.weight', 'model.layers.1.input_layernorm.weight', 'model.layers.1.post_attention_layernorm.weight', 'model.layers.2.self_attn.q_proj.weight', 'model.layers.2.self_attn.k_proj.weight', 'model.layers.2.self_attn.v_proj.weight', 'model.layers.2.self_attn.o_proj.weight', 'model.layers.2.mlp.gate_proj.weight', 'model.layers.2.mlp.up_proj.weight', 'model.layers.2.mlp.down_proj.weight', 'model.layers.2.input_layernorm.weight', 'model.layers.2.post_attention_layernorm.weight', 'model.layers.3.self_attn.q_proj.weight', 'model.layers.3.self_attn.k_proj.weight', 'model.layers.3.self_attn.v_proj.weight', 'model.layers.3.self_attn.o_proj.weight', 'model.layers.3.mlp.gate_proj.weight', 'model.layers.3.mlp.up_proj.weight', 'model.layers.3.mlp.down_proj.weight', 'model.layers.3.input_layernorm.weight', 'model.layers.3.post_attention_layernorm.weight', 'model.layers.4.self_attn.q_proj.weight', 'model.layers.4.self_attn.k_proj.weight', 'model.layers.4.self_attn.v_proj.weight', 'model.layers.4.self_attn.o_proj.weight', 'model.layers.4.mlp.gate_proj.weight', 'model.layers.4.mlp.up_proj.weight', 'model.layers.4.mlp.down_proj.weight', 'model.layers.4.input_layernorm.weight', 'model.layers.4.post_attention_layernorm.weight', 'model.layers.5.self_attn.q_proj.weight', 'model.layers.5.self_attn.k_proj.weight', 'model.layers.5.self_attn.v_proj.weight', 'model.layers.5.self_attn.o_proj.weight', 'model.layers.5.mlp.gate_proj.weight', 'model.layers.5.mlp.up_proj.weight', 'model.layers.5.mlp.down_proj.weight', 'model.layers.5.input_layernorm.weight', 'model.layers.5.post_attention_layernorm.weight', 'model.layers.6.self_attn.q_proj.weight', 'model.layers.6.self_attn.k_proj.weight', 'model.layers.6.self_attn.v_proj.weight', 'model.layers.6.self_attn.o_proj.weight', 'model.layers.6.mlp.gate_proj.weight', 'model.layers.6.mlp.up_proj.weight', 'model.layers.6.mlp.down_proj.weight', 'model.layers.6.input_layernorm.weight', 'model.layers.6.post_attention_layernorm.weight', 'model.layers.7.self_attn.q_proj.weight', 'model.layers.7.self_attn.k_proj.weight', 'model.layers.7.self_attn.v_proj.weight', 'model.layers.7.self_attn.o_proj.weight', 'model.layers.7.mlp.gate_proj.weight', 'model.layers.7.mlp.up_proj.weight', 'model.layers.7.mlp.down_proj.weight', 'model.layers.7.input_layernorm.weight', 'model.layers.7.post_attention_layernorm.weight', 'model.layers.8.self_attn.q_proj.weight', 'model.layers.8.self_attn.k_proj.weight', 'model.layers.8.self_attn.v_proj.weight', 'model.layers.8.self_attn.o_proj.weight', 'model.layers.8.mlp.gate_proj.weight', 'model.layers.8.mlp.up_proj.weight', 'model.layers.8.mlp.down_proj.weight', 'model.layers.8.input_layernorm.weight', 'model.layers.8.post_attention_layernorm.weight', 'model.layers.9.self_attn.q_proj.weight', 'model.layers.9.self_attn.k_proj.weight', 'model.layers.9.self_attn.v_proj.weight', 'model.layers.9.self_attn.o_proj.weight', 'model.layers.9.mlp.gate_proj.weight', 'model.layers.9.mlp.up_proj.weight', 'model.layers.9.mlp.down_proj.weight', 'model.layers.9.input_layernorm.weight', 'model.layers.9.post_attention_layernorm.weight', 'model.layers.10.self_attn.q_proj.weight', 'model.layers.10.self_attn.k_proj.weight', 'model.layers.10.self_attn.v_proj.weight', 'model.layers.10.self_attn.o_proj.weight', 'model.layers.10.mlp.gate_proj.weight', 'model.layers.10.mlp.up_proj.weight', 'model.layers.10.mlp.down_proj.weight', 'model.layers.10.input_layernorm.weight', 'model.layers.10.post_attention_layernorm.weight', 'model.layers.11.self_attn.q_proj.weight', 'model.layers.11.self_attn.k_proj.weight', 'model.layers.11.self_attn.v_proj.weight', 'model.layers.11.self_attn.o_proj.weight', 'model.layers.11.mlp.gate_proj.weight', 'model.layers.11.mlp.up_proj.weight', 'model.layers.11.mlp.down_proj.weight', 'model.layers.11.input_layernorm.weight', 'model.layers.11.post_attention_layernorm.weight', 'model.layers.12.self_attn.q_proj.weight', 'model.layers.12.self_attn.k_proj.weight', 'model.layers.12.self_attn.v_proj.weight', 'model.layers.12.self_attn.o_proj.weight', 'model.layers.12.mlp.gate_proj.weight', 'model.layers.12.mlp.up_proj.weight', 'model.layers.12.mlp.down_proj.weight', 'model.layers.12.input_layernorm.weight', 'model.layers.12.post_attention_layernorm.weight', 'model.layers.13.self_attn.q_proj.weight', 'model.layers.13.self_attn.k_proj.weight', 'model.layers.13.self_attn.v_proj.weight', 'model.layers.13.self_attn.o_proj.weight', 'model.layers.13.mlp.gate_proj.weight', 'model.layers.13.mlp.up_proj.weight', 'model.layers.13.mlp.down_proj.weight', 'model.layers.13.input_layernorm.weight', 'model.layers.13.post_attention_layernorm.weight', 'model.layers.14.self_attn.q_proj.weight', 'model.layers.14.self_attn.k_proj.weight', 'model.layers.14.self_attn.v_proj.weight', 'model.layers.14.self_attn.o_proj.weight', 'model.layers.14.mlp.gate_proj.weight', 'model.layers.14.mlp.up_proj.weight', 'model.layers.14.mlp.down_proj.weight', 'model.layers.14.input_layernorm.weight', 'model.layers.14.post_attention_layernorm.weight', 'model.layers.15.self_attn.q_proj.weight', 'model.layers.15.self_attn.k_proj.weight', 'model.layers.15.self_attn.v_proj.weight', 'model.layers.15.self_attn.o_proj.weight', 'model.layers.15.mlp.gate_proj.weight', 'model.layers.15.mlp.up_proj.weight', 'model.layers.15.mlp.down_proj.weight', 'model.layers.15.input_layernorm.weight', 'model.layers.15.post_attention_layernorm.weight', 'model.layers.16.self_attn.q_proj.weight', 'model.layers.16.self_attn.k_proj.weight', 'model.layers.16.self_attn.v_proj.weight', 'model.layers.16.self_attn.o_proj.weight', 'model.layers.16.mlp.gate_proj.weight', 'model.layers.16.mlp.up_proj.weight', 'model.layers.16.mlp.down_proj.weight', 'model.layers.16.input_layernorm.weight', 'model.layers.16.post_attention_layernorm.weight', 'model.layers.17.self_attn.q_proj.weight', 'model.layers.17.self_attn.k_proj.weight', 'model.layers.17.self_attn.v_proj.weight', 'model.layers.17.self_attn.o_proj.weight', 'model.layers.17.mlp.gate_proj.weight', 'model.layers.17.mlp.up_proj.weight', 'model.layers.17.mlp.down_proj.weight', 'model.layers.17.input_layernorm.weight', 'model.layers.17.post_attention_layernorm.weight', 'model.layers.18.self_attn.q_proj.weight', 'model.layers.18.self_attn.k_proj.weight', 'model.layers.18.self_attn.v_proj.weight', 'model.layers.18.self_attn.o_proj.weight', 'model.layers.18.mlp.gate_proj.weight', 'model.layers.18.mlp.up_proj.weight', 'model.layers.18.mlp.down_proj.weight', 'model.layers.18.input_layernorm.weight', 'model.layers.18.post_attention_layernorm.weight', 'model.layers.19.self_attn.q_proj.weight', 'model.layers.19.self_attn.k_proj.weight', 'model.layers.19.self_attn.v_proj.weight', 'model.layers.19.self_attn.o_proj.weight', 'model.layers.19.mlp.gate_proj.weight', 'model.layers.19.mlp.up_proj.weight', 'model.layers.19.mlp.down_proj.weight', 'model.layers.19.input_layernorm.weight', 'model.layers.19.post_attention_layernorm.weight', 'model.layers.20.self_attn.q_proj.weight', 'model.layers.20.self_attn.k_proj.weight', 'model.layers.20.self_attn.v_proj.weight', 'model.layers.20.self_attn.o_proj.weight', 'model.layers.20.mlp.gate_proj.weight', 'model.layers.20.mlp.up_proj.weight', 'model.layers.20.mlp.down_proj.weight', 'model.layers.20.input_layernorm.weight', 'model.layers.20.post_attention_layernorm.weight', 'model.layers.21.self_attn.q_proj.weight', 'model.layers.21.self_attn.k_proj.weight', 'model.layers.21.self_attn.v_proj.weight', 'model.layers.21.self_attn.o_proj.weight', 'model.layers.21.mlp.gate_proj.weight', 'model.layers.21.mlp.up_proj.weight', 'model.layers.21.mlp.down_proj.weight', 'model.layers.21.input_layernorm.weight', 'model.layers.21.post_attention_layernorm.weight', 'model.layers.22.self_attn.q_proj.weight', 'model.layers.22.self_attn.k_proj.weight', 'model.layers.22.self_attn.v_proj.weight', 'model.layers.22.self_attn.o_proj.weight', 'model.layers.22.mlp.gate_proj.weight', 'model.layers.22.mlp.up_proj.weight', 'model.layers.22.mlp.down_proj.weight', 'model.layers.22.input_layernorm.weight', 'model.layers.22.post_attention_layernorm.weight', 'model.layers.23.self_attn.q_proj.weight', 'model.layers.23.self_attn.k_proj.weight', 'model.layers.23.self_attn.v_proj.weight', 'model.layers.23.self_attn.o_proj.weight', 'model.layers.23.mlp.gate_proj.weight', 'model.layers.23.mlp.up_proj.weight', 'model.layers.23.mlp.down_proj.weight', 'model.layers.23.input_layernorm.weight', 'model.layers.23.post_attention_layernorm.weight', 'model.layers.24.self_attn.q_proj.weight', 'model.layers.24.self_attn.k_proj.weight', 'model.layers.24.self_attn.v_proj.weight', 'model.layers.24.self_attn.o_proj.weight', 'model.layers.24.mlp.gate_proj.weight', 'model.layers.24.mlp.up_proj.weight', 'model.layers.24.mlp.down_proj.weight', 'model.layers.24.input_layernorm.weight', 'model.layers.24.post_attention_layernorm.weight', 'model.layers.25.self_attn.q_proj.weight', 'model.layers.25.self_attn.k_proj.weight', 'model.layers.25.self_attn.v_proj.weight', 'model.layers.25.self_attn.o_proj.weight', 'model.layers.25.mlp.gate_proj.weight', 'model.layers.25.mlp.up_proj.weight', 'model.layers.25.mlp.down_proj.weight', 'model.layers.25.input_layernorm.weight', 'model.layers.25.post_attention_layernorm.weight', 'model.layers.26.self_attn.q_proj.weight', 'model.layers.26.self_attn.k_proj.weight', 'model.layers.26.self_attn.v_proj.weight', 'model.layers.26.self_attn.o_proj.weight', 'model.layers.26.mlp.gate_proj.weight', 'model.layers.26.mlp.up_proj.weight', 'model.layers.26.mlp.down_proj.weight', 'model.layers.26.input_layernorm.weight', 'model.layers.26.post_attention_layernorm.weight', 'model.layers.27.self_attn.q_proj.weight', 'model.layers.27.self_attn.k_proj.weight', 'model.layers.27.self_attn.v_proj.weight', 'model.layers.27.self_attn.o_proj.weight', 'model.layers.27.mlp.gate_proj.weight', 'model.layers.27.mlp.up_proj.weight', 'model.layers.27.mlp.down_proj.weight', 'model.layers.27.input_layernorm.weight', 'model.layers.27.post_attention_layernorm.weight', 'model.layers.28.self_attn.q_proj.weight', 'model.layers.28.self_attn.k_proj.weight', 'model.layers.28.self_attn.v_proj.weight', 'model.layers.28.self_attn.o_proj.weight', 'model.layers.28.mlp.gate_proj.weight', 'model.layers.28.mlp.up_proj.weight', 'model.layers.28.mlp.down_proj.weight', 'model.layers.28.input_layernorm.weight', 'model.layers.28.post_attention_layernorm.weight', 'model.layers.29.self_attn.q_proj.weight', 'model.layers.29.self_attn.k_proj.weight', 'model.layers.29.self_attn.v_proj.weight', 'model.layers.29.self_attn.o_proj.weight', 'model.layers.29.mlp.gate_proj.weight', 'model.layers.29.mlp.up_proj.weight', 'model.layers.29.mlp.down_proj.weight', 'model.layers.29.input_layernorm.weight', 'model.layers.29.post_attention_layernorm.weight', 'model.layers.30.self_attn.q_proj.weight', 'model.layers.30.self_attn.k_proj.weight', 'model.layers.30.self_attn.v_proj.weight', 'model.layers.30.self_attn.o_proj.weight', 'model.layers.30.mlp.gate_proj.weight', 'model.layers.30.mlp.up_proj.weight', 'model.layers.30.mlp.down_proj.weight', 'model.layers.30.input_layernorm.weight', 'model.layers.30.post_attention_layernorm.weight', 'model.layers.31.self_attn.q_proj.weight', 'model.layers.31.self_attn.k_proj.weight', 'model.layers.31.self_attn.v_proj.weight', 'model.layers.31.self_attn.o_proj.weight', 'model.layers.31.mlp.gate_proj.weight', 'model.layers.31.mlp.up_proj.weight', 'model.layers.31.mlp.down_proj.weight', 'model.layers.31.input_layernorm.weight', 'model.layers.31.post_attention_layernorm.weight', 'model.norm.weight', 'lm_head.weight'])\n"
          ]
        }
      ],
      "source": [
        "# Print list of modules\n",
        "print(model.state_dict().keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cIxNeO4Ez9Q2",
        "outputId": "116a22e4-5564-4a80-cf4e-9f55a8f74d00",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralAttention(\n",
            "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VAPRl71yz9Q3",
        "outputId": "d8ac3aa7-4a6e-4b3e-f355-752be3ec39c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20971520 || all params: 7262703616 || trainable %: 0.2887563792882719\n"
          ]
        }
      ],
      "source": [
        "# # If extending model context\n",
        "# def set_added_trainable_params(model):\n",
        "#     \"\"\"\n",
        "#     Sets the parameters with names containing \"embed\" or \"norm\" as trainable.\n",
        "#     \"\"\"\n",
        "#     trainable_params_dict = {}\n",
        "\n",
        "#     for name, param in model.named_parameters():\n",
        "#         if \"embed\" in name or \"norm\" in name: #for most models\n",
        "#         # if \"ln\" in name or \"embd\" in name: #for Phi-2\n",
        "#             param.requires_grad_()\n",
        "#             trainable_params_dict[name] = param\n",
        "\n",
        "#     return trainable_params_dict\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable %: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "# Initialize LoRA configuration\n",
        "config = LoraConfig(\n",
        "    # Lower rank results in smaller update matrices with fewer trainable params\n",
        "    r=8, # Use 8 for models >=7B or larger, else 128\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\n",
        "    #     \"Wqkv\", #for Phi-2\n",
        "    #     \"fc1\", #for Phi-2\n",
        "    #     \"fc2\" #for Phi-2\n",
        "      \"self_attn.q_proj\",\n",
        "      \"self_attn.k_proj\",\n",
        "      \"self_attn.v_proj\",\n",
        "      \"self_attn.o_proj\",\n",
        "      # \"self_attn.rotary_emb.inv_freq\",\n",
        "      \"mlp.gate_proj\",\n",
        "      \"mlp.up_proj\",\n",
        "      \"mlp.down_proj\",\n",
        "      # \"input_layernorm.weight\",\n",
        "      # \"post_attention_layernorm.weight\",\n",
        "      # \"model.norm.weight\",\n",
        "      # \"lm_head.weight\"\n",
        "    ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# Apply LoRA to the model\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# # Set added parameters with names containing \"embed\" or \"norm\" as trainable.\n",
        "# # Recommended if you are extending an LLM's context window.\n",
        "# set_added_trainable_params(model)\n",
        "\n",
        "# Print out the number of trainable parameters\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxDuoyQvQfD2"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each function in the data set is stored as JSON in its own file. All functions follow OpenAI's metadata format.\n",
        "\n",
        "### JSON data format\n",
        "``` json\n",
        "{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"function_name\",\n",
        "        \"description\": \"function description\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"property_1\": {\n",
        "                    \"type\": \"property_type\", //#e.g. string\n",
        "                    \"description\": \"property description\"\n",
        "                },\n",
        "                \"property_2\": {\n",
        "                    \"type\": \"property_type\", //#e.g. string\n",
        "                    \"description\": \"property description\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"property_1\",\"property_2\"]\n",
        "        }\n",
        "    },\n",
        "    \"samplePromptResponsePairs\": [\n",
        "        {\n",
        "            \"prompt\": \"sample_prompt\",\n",
        "            \"response\": {\n",
        "                \"name\": \"generate_password\",\n",
        "                \"arguments\": {\n",
        "                    \"property_1\": \"property_value\",\n",
        "                    \"property_2\": \"property_value\"\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        ...\n",
        "    ]\n",
        "}\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "iCJcitb6vkr8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Cc3LsTLFjAGT",
        "outputId": "d3065ac1-8be7-403a-d52d-dbc8b30151fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "UZbrRk4J3qVH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273,
          "referenced_widgets": [
            "fd4baa0310e04185aa66cd059e11b634",
            "ce8f559fe604435496b033dfd587be36",
            "9fdccd859cfe4ccdaa84d14a9e5e9f68",
            "6d519af3fa684d04a50a23261330c3aa",
            "02030878b6e24e27bce39c443cd0f449",
            "3cd6f31e486a4593abd4c72fd294246b",
            "853b9bcdea6f447ba69390fa1657f018",
            "03afa23d3f86422997fdfac4a3aaba27",
            "4daeeed5fdb2496fbf23b464f8ee01f7",
            "a2f263ac79db425e8f0e33a03a91ce32",
            "227f4f335a134779b1ee1f0ddde0828a",
            "0cd67f0d4330435e834da748813027e3",
            "4d89743558af489b9102c81d2070cbba",
            "a3daec5d9da64f73b661e9d9e952cfef",
            "2203bbad2b164e16b7d79bdacffcd776",
            "b9a74e7a02c943528d82de6193531cf0",
            "a21c579a7881499e94ef5b38824795ec",
            "79cf11ee187345ce8e5c897b0c44bbb1",
            "9b7e0030b7044795bfc6f9e9266c109a",
            "b7bf52931baf49f4b0a76abe82af76b0",
            "89a7b4a6c1b4447fbb8069e5835428db",
            "5e178a1822d9450192ba82e229d29faf",
            "40f327f8817a4baea398bbbb71c29e33",
            "dbe9d0685c964499b9d1d6e089f497c2",
            "e4e20f042a4d4510ac5ed7d165b48d42",
            "b132e9258a844190b9dea38caad0a140",
            "4d9a408d7c8947b6b072aeda8998de38",
            "02e5699e25134e099d6f057112aec8d6",
            "40953a0c20084f76ab05bb8ddd4623b8",
            "518ab55044f0417a989961ae2f8b0dc5",
            "4e6249227d53488285c87b70be1250d9",
            "387ee85d6b2f499f807570c833d558af",
            "5937446d5e464e609f290b4a214a1dae",
            "63a0614691ae44699a137309b010c454",
            "60e3d53e60cd42929b91b159f3e2d739",
            "32ff3908b9ca42a0859cb5b9cc7f2c6b",
            "652b42942f5347af945305dd07b5f312",
            "216b95e365184604b1a2066f2e991bae",
            "2f114824c0ec49bc86214c9edb6efbcf",
            "caa9cb6531a847ebb958ec135c7f6417",
            "d7e9c1786f3c4d888cb364c19a3d544f",
            "f8f98aec7f754696b19b297ab58bf016",
            "3c135a31bfcb4154829c3cf30ac992f2",
            "3392a9b672274a578000a74babf37d19",
            "fa10b163c1a243e98c21cfd751383750",
            "cd2fbdd3cffe40f0ad22f043775bc4f8",
            "176f5d464d5443c6af576ff125bdc3f5",
            "2d0a34ffbc27446aa2150d17c581643d",
            "aee1b0ad891d48b7bfced2738cf6386f",
            "4d28d4aad81d4986b49a2db737e5ed5c",
            "96834578d28e46bba388916250ac2bba",
            "93749d17621f4407b6645b5d31533a3f",
            "44d67fa2f435462097695940978f1687",
            "2b31fcb97d3745a081aeb3ee348982f2",
            "65edadf394cd42d5a0bb76c2d3050b55",
            "fc8eb5315a0e47089d932c3680d765fc",
            "eb91b25442714f64a15f851cf5be9c32",
            "395761eed53c4d55a24f8c402c40ef1f",
            "a3fd0be2f4bc4195936097c139ca6a27",
            "028aab08784142f1af036fa8830885b4",
            "36c2f6fe1958442aa2867b655509e37f",
            "884be5e0c48b45899e5fe8e445832c1b",
            "f32a95235e9b4577993c05e743121eca",
            "02ec9ec693214dea94f0312c95dbe65d",
            "9edb043110b84f2e8daa025c85938f0e",
            "1f6c4e3a461b47f1bc8adf48cdb1cde2",
            "a19603c93def428a8701fe1990c8ab45",
            "3633799ae6864bf098fc2ee65f686ed5",
            "6a62f2f6ca89409194ffb75ebbba102a",
            "5037ce6b3b864fa5ad4f9111954a8d4e",
            "a085a4f753a945bfaebb904841d1dbc7",
            "1d1ae418423343898a5dcf136b42a412",
            "657c8a8eea1e4129b4e4e286fddbeef5",
            "5ed9e7f10a1c49bfb6caec54615c8fc5",
            "662bf06140894a7d9459465f137a2e26",
            "d4ec19f199b5417f8d00d345e7a0d910",
            "6a5595e8361a4c67a0e31c05452edd50",
            "613f5d2b77ca4ee4a2351fc3af8b9da7",
            "37a7d04b3db7499eab781a285f5bc2e0",
            "ceeea19cfbad42079465ccee698cf574",
            "9c46cd15ed204f0fa0f3ba85a7d8d24a",
            "3d8ac7159f394eb09cb5cde55aeecea3",
            "8ad98975e1214d88a01590fd95c616c9",
            "a3b49da203194279986bcbce3c630ce4",
            "9c248781d6c24f88ad9d1555314aea11",
            "95d753afcf724120bfde10b2ef39bdec",
            "021daab07db04423b4d6366412320dc3",
            "041172698acc4682b3127343e16b8446"
          ]
        },
        "outputId": "8f4aab77-f242-4892-bd24-102ccd148c2d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/8.93k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fd4baa0310e04185aa66cd059e11b634"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/104k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0cd67f0d4330435e834da748813027e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/7.83k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40f327f8817a4baea398bbbb71c29e33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/32.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "63a0614691ae44699a137309b010c454"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/11.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa10b163c1a243e98c21cfd751383750"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc8eb5315a0e47089d932c3680d765fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating validation split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a19603c93def428a8701fe1990c8ab45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "613f5d2b77ca4ee4a2351fc3af8b9da7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# From Hugging Face Hub\n",
        "data = load_dataset(\n",
        "    \"Trelis/function_calling_v3\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIVJPMTBAzID",
        "outputId": "666677a7-f489-4492-ec0b-7ed28260dafa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['functionList', 'userPrompt', 'assistantResponse'],\n",
            "        num_rows: 66\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['functionList', 'userPrompt', 'assistantResponse'],\n",
            "        num_rows: 19\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['functionList', 'userPrompt', 'assistantResponse'],\n",
            "        num_rows: 7\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "nNyFtLkj1G5f"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, response_lengths, input_lengths):\n",
        "        self.encodings = encodings\n",
        "        self.response_lengths = response_lengths\n",
        "        self.input_lengths = input_lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "\n",
        "        # Set labels to be the same as input_ids\n",
        "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "\n",
        "        # Calculate the start and end positions of the response\n",
        "        response_start_position = self.input_lengths[idx]\n",
        "        response_end_position = self.input_lengths[idx] + self.response_lengths[idx]\n",
        "\n",
        "        # Create a loss mask that covers only the response tokens\n",
        "        item[\"loss_mask\"] = torch.zeros_like(item[\"input_ids\"])\n",
        "        item[\"loss_mask\"][response_start_position:response_end_position] = 1\n",
        "\n",
        "        # Shift the loss mask to the left by one position\n",
        "        shifted_loss_mask = torch.cat([item[\"loss_mask\"][1:], torch.tensor([0])])\n",
        "        item[\"loss_mask\"] = shifted_loss_mask\n",
        "\n",
        "        # Shift the labels to the left by one position\n",
        "        item[\"labels\"][:-1] = item[\"input_ids\"][1:]\n",
        "\n",
        "        # Replace the token after the response with an EOS token\n",
        "        item[\"labels\"][response_end_position - 1] = tokenizer.eos_token_id\n",
        "\n",
        "        # Replace the token after the response with an 1 in the loss mask\n",
        "        item[\"loss_mask\"][response_end_position - 1] = 1\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "dgdX5vtRz9Q4"
      },
      "outputs": [],
      "source": [
        "# Define the function start and end strings\n",
        "# \\n\\n is added at the end during training to avoid different tokenizations of\n",
        "# the E_INST string with whatever follows.\n",
        "B_FUNC, E_FUNC = \"You have access to the following functions. Use them if required:\\n\\n\", \"\\n\\n\"\n",
        "\n",
        "# Define the user prompt start and end strings\n",
        "# B_INST, E_INST = \"GPT4 Correct User: \", \"<|end_of_turn|>GPT4 Correct Assistant:\" # OpenChat style\n",
        "B_INST, E_INST = \"[INST] \", \" [/INST]\" # Llama 2 or Mistral style\n",
        "# B_INST, E_INST = \"Instruct:\", \"\\nOutput:\" # Phi 2\n",
        "# B_INST, E_INST = \"\\n### Instruction:\\n\", \"\\n### Response:\\n\" # DeepSeek Coder style\n",
        "# B_INST, E_INST = \"Human: \", \" Assistant:\" # Yi style for function calling, no training space\n",
        "# B_INST, E_INST = \"### Human: \", \"\\n\\n### Assistant: \" # SUSChat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ewsa4W2Z1HfB"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(dataset, tokenizer):\n",
        "    # Create the formatted text with the correct roles for each part of the dialogue\n",
        "    formatted_dataset = dataset.map(\n",
        "        lambda x: {\n",
        "            \"input_text\": \"\".join([\n",
        "                f\"{B_INST}{B_FUNC}{x['functionList'].strip()}{E_FUNC}\",\n",
        "                f\"{x['userPrompt'].strip()}{E_INST}\\n\\n\",\n",
        "                f\"{x['assistantResponse'].strip()}\",  # append EOS token in TextData...\n",
        "            ]),\n",
        "            \"response_text\": \"\".join([\n",
        "                f\"{x['assistantResponse'].strip()}\",  # append EOS token in TextData...\n",
        "            ]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Tokenize the datasets\n",
        "    encodings = tokenizer([dialogue[\"input_text\"] for dialogue in \\\n",
        "                           formatted_dataset], truncation=True, padding=True, \\\n",
        "                          max_length=1024, return_tensors='pt', \\\n",
        "                          add_special_tokens=True)\n",
        "\n",
        "    # Tokenize the response one by one without padding and special tokens for\n",
        "    # the purpose of calculating length\n",
        "    response_lengths = [len(tokenizer.encode(dialogue[\"response_text\"], \\\n",
        "                                             truncation=True, max_length=1024, \\\n",
        "                                             padding=False, \\\n",
        "                                             add_special_tokens=False)) \\\n",
        "                        for dialogue in formatted_dataset]\n",
        "\n",
        "    # Tokenize the input one by one without padding and with the initial\n",
        "    # special token for the purpose of calculating length\n",
        "    total_lengths = [len(tokenizer.encode(dialogue[\"input_text\"], \\\n",
        "                                          truncation=True, max_length=1024, \\\n",
        "                                          padding=False, \\\n",
        "                                          add_special_tokens=True)) \\\n",
        "                     for dialogue in formatted_dataset]\n",
        "    input_lengths = [total_length - response_length \\\n",
        "                     for total_length, response_length in \\\n",
        "                     zip(total_lengths, response_lengths)]\n",
        "\n",
        "    # Create TextDataset\n",
        "    text_dataset = TextDataset(encodings, response_lengths, input_lengths)\n",
        "\n",
        "    return text_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "WRB550G01J5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "f85c2971871946e98d3e92e9553aa2ff",
            "076389ae7bc34bc98a07d8e59a1b7d3d",
            "9202f873e7ff47efb96baaad1aeec5b8",
            "3fad8ca09a6b4d8ebfd9cb3f7798d99d",
            "4ba58c66a81a423ab6fe74058fe059b8",
            "78492bc74aaa4360ae37914c96778a57",
            "524f38bd7e31488cbfe3a04f4e20d8ac",
            "60402cc8dd8245928a453be36fe3ef8c",
            "03d92454b18043509358f3195f569764",
            "cf5f69af7a864718b953e625c5a18490",
            "ba7f8f1da751475a9455e58f44e2791c",
            "bc06f1b48cb3464491d1d63fe4188f1c",
            "bee8ae5d3e67421e903c7f399fa7acf6",
            "f23e13b27f4b4944bac0df766f5720df",
            "4b7976f2d5b146efb57b5d2d0e5d7608",
            "9e28024abd8e4933b05e05f09e0db4db",
            "02f833c862d44ed198870976309a7761",
            "312519c6c64e4a1cb7a07373c2a2a6ff",
            "71c5d52afa9049288ca4d4840b3e9476",
            "a250b6b29cc74a99bcc1aec5109fd1ea",
            "32ff85599f8d41e89d288e94a7284461",
            "96f15abfc9da487eab4ca71f33402035",
            "6aea10eb6a224e0c810f85eec89ce3ca",
            "a144a1591aca418c857bb529d30c6c47",
            "fc18adc18ff04abfb32830a3cf79ed0e",
            "f1fae15e173149d1a53e40339c20e95b",
            "66e4c2e9340e4b9eac87f67d6222d2cf",
            "7358f2ac4bb740f7ba565d20e9aab7ea",
            "a1051eda0b864f598a06315b11575d73",
            "9ec01b900dcf46e2bb7f97f10af61dcb",
            "76d5f487254c4c5f840e24c7b10447ca",
            "57a9900ca9d84b7284acf105a7edb31c",
            "f042f28f7b4440a6af003c2e5e06aa38"
          ]
        },
        "outputId": "ba812fc9-9aea-40e8-b0b3-567b9e396e73"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/66 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f85c2971871946e98d3e92e9553aa2ff"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc06f1b48cb3464491d1d63fe4188f1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/19 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aea10eb6a224e0c810f85eec89ce3ca"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Apply function to your datasets\n",
        "train_dataset = prepare_dataset(data['train'], tokenizer)\n",
        "test_dataset = prepare_dataset(data['test'], tokenizer)\n",
        "validation_dataset = prepare_dataset(data['validation'], tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLmXcTTV2RdD"
      },
      "source": [
        "### Examine the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "YaR9H_pH2fOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6741f2d-f7d8-493a-de57-925e970bd794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples in the dataset: 66\n",
            "Dimensions of input_ids: torch.Size([677])\n",
            "Dimensions of attention_mask: torch.Size([677])\n",
            "Dimensions of loss_mask: torch.Size([677])\n",
            "Dimensions of labels: torch.Size([677])\n",
            "\n",
            "Tokens at the start of the sample:\n",
            "[1, 733, 16289, 28793, 995, 506, 2735, 298, 272, 2296, 5572, 28723, 5938, 706, 513, 3030, 28747, 13, 13, 28792, 13, 2287, 371, 13, 5390, 345, 1123, 1264, 345, 2628, 548, 13, 5390, 345, 2628, 1264, 371, 13, 17422, 345, 861, 1264, 345, 2360, 28730, 283, 28744, 449, 548, 13, 17422, 345, 6518, 1264, 345, 7009, 354, 3332, 10374, 356, 1010, 28814, 449, 28723, 6746, 938, 302, 5771, 28725, 3994, 304, 5457, 12765, 390, 7658, 298, 5175, 3471, 2373, 272, 5709, 9191, 13, 17422, 345, 11438, 1264, 371, 13, 1417, 28705, 345, 1123, 1264, 345, 2814, 548, 13, 1417, 28705, 345, 10723, 1264, 371, 13, 359, 2287, 345, 3385, 1264, 371, 13, 359, 5390, 345, 1123, 1264, 345, 1427, 548, 13, 359, 5390, 345, 6518, 1264, 345, 1014, 3472, 5709, 1423, 28739, 13, 359, 2287, 443, 13, 1417, 28705, 1630, 13, 1417, 28705, 345, 10893, 1264, 733, 13, 359, 2287, 345, 3385, 28739, 13, 1417, 28705, 4709, 13, 17422, 443, 13, 5390, 443, 13, 2287, 1630, 13, 2287, 371, 13, 5390, 345, 1123, 1264, 345, 2628, 548, 13, 5390, 345, 2628, 1264, 371, 13, 17422, 345, 861, 1264, 345, 527, 28730, 3022, 28730, 769, 1223, 548, 13, 17422, 345, 6518, 1264, 345, 1458, 272, 1868, 8086, 297, 264, 2078, 4723, 548, 13, 17422, 345, 11438, 1264, 371, 13, 1417, 28705, 345, 1123, 1264, 345, 2814, 548, 13, 1417, 28705, 345, 10723, 1264, 371, 13, 359, 2287, 345, 2733, 1264, 371, 13, 359, 5390, 345, 1123, 1264, 345, 1427, 548, 13, 359, 5390, 345, 6518, 1264, 345, 1014, 2990, 304, 2939, 28725, 317, 28723, 28721, 28723, 22263, 28725, 11170, 28739, 13, 359, 2287, 1630, 13, 359, 2287, 345, 5306, 1264, 371, 13, 359, 5390, 345, 1123, 1264, 345, 1427, 548, 13, 359, 5390, 345, 6518, 1264, 345, 25241, 466, 5028, 354, 272, 8086, 28723, 19641, 28747, 464, 28717, 1190, 3170, 647, 464, 28722, 18657, 12307, 21236, 13, 359, 2287, 443, 13, 1417, 28705, 1630, 13, 1417, 28705, 345, 10893, 1264, 733, 13, 359, 2287, 345, 2733, 28739]\n",
            "['<s>', '▁[', 'INST', ']', '▁You', '▁have', '▁access', '▁to', '▁the', '▁following', '▁functions', '.', '▁Use', '▁them', '▁if', '▁required', ':', '<0x0A>', '<0x0A>', '[', '<0x0A>', '▁▁▁', '▁{', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'function', '\",', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'function', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'name', '\":', '▁\"', 'search', '_', 'ar', 'x', 'iv', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'Search', '▁for', '▁research', '▁papers', '▁on', '▁Ar', 'X', 'iv', '.', '▁Make', '▁use', '▁of', '▁AND', ',', '▁OR', '▁and', '▁NOT', '▁operators', '▁as', '▁appropriate', '▁to', '▁join', '▁terms', '▁within', '▁the', '▁query', '.\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'parameters', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'type', '\":', '▁\"', 'object', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'properties', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'query', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'string', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'The', '▁search', '▁query', '▁string', '\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁}', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁},', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'required', '\":', '▁[', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'query', '\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁]', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁}', '<0x0A>', '▁▁▁▁▁▁▁', '▁}', '<0x0A>', '▁▁▁', '▁},', '<0x0A>', '▁▁▁', '▁{', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'function', '\",', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'function', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'name', '\":', '▁\"', 'get', '_', 'current', '_', 'we', 'ather', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'Get', '▁the', '▁current', '▁weather', '▁in', '▁a', '▁given', '▁location', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'parameters', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'type', '\":', '▁\"', 'object', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'properties', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'location', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'string', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'The', '▁city', '▁and', '▁country', ',', '▁e', '.', 'g', '.', '▁Dublin', ',', '▁Ireland', '\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁},', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'unit', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'string', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'Measure', 'ment', '▁unit', '▁for', '▁the', '▁weather', '.', '▁Options', ':', \"▁'\", 'c', 'els', 'ius', \"',\", \"▁'\", 'f', 'ahren', 'heit', '\\'\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁}', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁},', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'required', '\":', '▁[', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'location', '\"']\n",
            "\n",
            "Labels at the start of the sample:\n",
            "[733, 16289, 28793, 995, 506, 2735, 298, 272, 2296, 5572, 28723, 5938, 706, 513, 3030, 28747, 13, 13, 28792, 13, 2287, 371, 13, 5390, 345, 1123, 1264, 345, 2628, 548, 13, 5390, 345, 2628, 1264, 371, 13, 17422, 345, 861, 1264, 345, 2360, 28730, 283, 28744, 449, 548, 13, 17422, 345, 6518, 1264, 345, 7009, 354, 3332, 10374, 356, 1010, 28814, 449, 28723, 6746, 938, 302, 5771, 28725, 3994, 304, 5457, 12765, 390, 7658, 298, 5175, 3471, 2373, 272, 5709, 9191, 13, 17422, 345, 11438, 1264, 371, 13, 1417, 28705, 345, 1123, 1264, 345, 2814, 548, 13, 1417, 28705, 345, 10723, 1264, 371, 13, 359, 2287, 345, 3385, 1264, 371, 13, 359, 5390, 345, 1123, 1264, 345, 1427, 548, 13, 359, 5390, 345, 6518, 1264, 345, 1014, 3472, 5709, 1423, 28739, 13, 359, 2287, 443, 13, 1417, 28705, 1630, 13, 1417, 28705, 345, 10893, 1264, 733, 13, 359, 2287, 345, 3385, 28739, 13, 1417, 28705, 4709, 13, 17422, 443, 13, 5390, 443, 13, 2287, 1630, 13, 2287, 371, 13, 5390, 345, 1123, 1264, 345, 2628, 548, 13, 5390, 345, 2628, 1264, 371, 13, 17422, 345, 861, 1264, 345, 527, 28730, 3022, 28730, 769, 1223, 548, 13, 17422, 345, 6518, 1264, 345, 1458, 272, 1868, 8086, 297, 264, 2078, 4723, 548, 13, 17422, 345, 11438, 1264, 371, 13, 1417, 28705, 345, 1123, 1264, 345, 2814, 548, 13, 1417, 28705, 345, 10723, 1264, 371, 13, 359, 2287, 345, 2733, 1264, 371, 13, 359, 5390, 345, 1123, 1264, 345, 1427, 548, 13, 359, 5390, 345, 6518, 1264, 345, 1014, 2990, 304, 2939, 28725, 317, 28723, 28721, 28723, 22263, 28725, 11170, 28739, 13, 359, 2287, 1630, 13, 359, 2287, 345, 5306, 1264, 371, 13, 359, 5390, 345, 1123, 1264, 345, 1427, 548, 13, 359, 5390, 345, 6518, 1264, 345, 25241, 466, 5028, 354, 272, 8086, 28723, 19641, 28747, 464, 28717, 1190, 3170, 647, 464, 28722, 18657, 12307, 21236, 13, 359, 2287, 443, 13, 1417, 28705, 1630, 13, 1417, 28705, 345, 10893, 1264, 733, 13, 359, 2287, 345, 2733, 28739, 13]\n",
            "['▁[', 'INST', ']', '▁You', '▁have', '▁access', '▁to', '▁the', '▁following', '▁functions', '.', '▁Use', '▁them', '▁if', '▁required', ':', '<0x0A>', '<0x0A>', '[', '<0x0A>', '▁▁▁', '▁{', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'function', '\",', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'function', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'name', '\":', '▁\"', 'search', '_', 'ar', 'x', 'iv', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'Search', '▁for', '▁research', '▁papers', '▁on', '▁Ar', 'X', 'iv', '.', '▁Make', '▁use', '▁of', '▁AND', ',', '▁OR', '▁and', '▁NOT', '▁operators', '▁as', '▁appropriate', '▁to', '▁join', '▁terms', '▁within', '▁the', '▁query', '.\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'parameters', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'type', '\":', '▁\"', 'object', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'properties', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'query', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'string', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'The', '▁search', '▁query', '▁string', '\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁}', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁},', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'required', '\":', '▁[', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'query', '\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁]', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁}', '<0x0A>', '▁▁▁▁▁▁▁', '▁}', '<0x0A>', '▁▁▁', '▁},', '<0x0A>', '▁▁▁', '▁{', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'function', '\",', '<0x0A>', '▁▁▁▁▁▁▁', '▁\"', 'function', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'name', '\":', '▁\"', 'get', '_', 'current', '_', 'we', 'ather', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'Get', '▁the', '▁current', '▁weather', '▁in', '▁a', '▁given', '▁location', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁', '▁\"', 'parameters', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'type', '\":', '▁\"', 'object', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'properties', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'location', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'string', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'The', '▁city', '▁and', '▁country', ',', '▁e', '.', 'g', '.', '▁Dublin', ',', '▁Ireland', '\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁},', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'unit', '\":', '▁{', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'type', '\":', '▁\"', 'string', '\",', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁▁▁▁▁', '▁\"', 'description', '\":', '▁\"', 'Measure', 'ment', '▁unit', '▁for', '▁the', '▁weather', '.', '▁Options', ':', \"▁'\", 'c', 'els', 'ius', \"',\", \"▁'\", 'f', 'ahren', 'heit', '\\'\"', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁}', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁},', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁', '▁\"', 'required', '\":', '▁[', '<0x0A>', '▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁', '▁▁▁', '▁\"', 'location', '\"', '<0x0A>']\n",
            "Attention mask at the start of the sample:\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Loss mask at the start of the sample:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "Tokens at the end of the sample:\n",
            "[17422, 443, 13, 5390, 443, 13, 2287, 443, 13, 28793, 13, 13, 9607, 863, 19808, 15648, 2172, 4296, 28804, 733, 28748, 16289, 28793, 13, 13, 1014, 17008, 5016, 302, 19808, 15648, 349, 521, 6206, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['▁▁▁▁▁▁▁▁▁▁▁', '▁}', '<0x0A>', '▁▁▁▁▁▁▁', '▁}', '<0x0A>', '▁▁▁', '▁}', '<0x0A>', ']', '<0x0A>', '<0x0A>', 'Where', '▁did', '▁fortune', '▁cookies', '▁orig', 'inate', '?', '▁[', '/', 'INST', ']', '<0x0A>', '<0x0A>', 'The', '▁precise', '▁origin', '▁of', '▁fortune', '▁cookies', '▁is', '▁un', 'clear', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
            "\n",
            "Labels at the end of the sample:\n",
            "[443, 13, 5390, 443, 13, 2287, 443, 13, 28793, 13, 13, 9607, 863, 19808, 15648, 2172, 4296, 28804, 733, 28748, 16289, 28793, 13, 13, 1014, 17008, 5016, 302, 19808, 15648, 349, 521, 6206, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "['▁}', '<0x0A>', '▁▁▁▁▁▁▁', '▁}', '<0x0A>', '▁▁▁', '▁}', '<0x0A>', ']', '<0x0A>', '<0x0A>', 'Where', '▁did', '▁fortune', '▁cookies', '▁orig', 'inate', '?', '▁[', '/', 'INST', ']', '<0x0A>', '<0x0A>', 'The', '▁precise', '▁origin', '▁of', '▁fortune', '▁cookies', '▁is', '▁un', 'clear', '</s>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n",
            "Attention mask at the end of the sample:\n",
            "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "Loss mask at the end of the sample:\n",
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "# Print the number of items in the dataset\n",
        "print(f\"Number of samples in the dataset: {len(train_dataset)}\")\n",
        "\n",
        "# Get a sample item\n",
        "sample_item = train_dataset[1]  # replace with the index of any sample\n",
        "\n",
        "# Print the dimensions of the sample item\n",
        "print(f\"Dimensions of input_ids: {sample_item['input_ids'].shape}\")\n",
        "print(f\"Dimensions of attention_mask: {sample_item['attention_mask'].shape}\")\n",
        "print(f\"Dimensions of loss_mask: {sample_item['loss_mask'].shape}\")\n",
        "print(f\"Dimensions of labels: {sample_item['labels'].shape}\")\n",
        "\n",
        "# Print some tokens from the start and end of the sample\n",
        "num_tokens_to_print = 336  # replace with the number of tokens you want to print\n",
        "\n",
        "print(\"\\nTokens at the start of the sample:\")\n",
        "print(sample_item['input_ids'][:num_tokens_to_print].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['input_ids'][:num_tokens_to_print].tolist()))\n",
        "\n",
        "print(\"\\nLabels at the start of the sample:\")\n",
        "print(sample_item['labels'][:num_tokens_to_print].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['labels'][:num_tokens_to_print].tolist()))\n",
        "\n",
        "print(\"Attention mask at the start of the sample:\")\n",
        "print(sample_item['attention_mask'][:num_tokens_to_print].tolist())\n",
        "\n",
        "print(\"Loss mask at the start of the sample:\")\n",
        "print(sample_item['loss_mask'][:num_tokens_to_print].tolist())\n",
        "\n",
        "print(\"\\nTokens at the end of the sample:\")\n",
        "print(sample_item['input_ids'][-num_tokens_to_print:].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['input_ids'][-num_tokens_to_print:].tolist()))\n",
        "\n",
        "print(\"\\nLabels at the end of the sample:\")\n",
        "print(sample_item['labels'][-num_tokens_to_print:].tolist())\n",
        "print(tokenizer.convert_ids_to_tokens(sample_item['labels'][-num_tokens_to_print:].tolist()))\n",
        "\n",
        "print(\"Attention mask at the end of the sample:\")\n",
        "print(sample_item['attention_mask'][-num_tokens_to_print:].tolist())\n",
        "\n",
        "print(\"Loss mask at the end of the sample:\")\n",
        "print(sample_item['loss_mask'][-num_tokens_to_print:].tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6giTvOr2atI"
      },
      "source": [
        "# Generate a sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "dlZGi8K1Hio8"
      },
      "outputs": [],
      "source": [
        "import textwrap\n",
        "wrapper = textwrap.TextWrapper(width=80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "Hac875Cgz8Zz"
      },
      "outputs": [],
      "source": [
        "import re  # import regular expressions module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "IOhG_TJ9EWwz"
      },
      "outputs": [],
      "source": [
        "import gc  # import Python's garbage collection module\n",
        "\n",
        "def generate(index,data_split=\"test\"):\n",
        "\n",
        "    functionList = data[data_split][index]['functionList']\n",
        "    user_prompt = data[data_split][index]['userPrompt']\n",
        "    correct_answer = data[data_split][index]['assistantResponse']\n",
        "\n",
        "    # model.config.use_cache = True    # Unsure this is needed\n",
        "\n",
        "    # Format your prompt template\n",
        "    prompt = f\"{B_INST}{B_FUNC}{functionList.strip()}\\\n",
        "    {E_FUNC}{user_prompt.strip()}{E_INST}\\n\\n\"\n",
        "\n",
        "    print(f\"Using the {data_split} data split.\\n\\nPrompt:\")\n",
        "    print(prompt)\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    if \"token_type_ids\" in inputs:\n",
        "        del inputs[\"token_type_ids\"]\n",
        "\n",
        "    # print(f'model is on: {next(model.parameters()).device}')  # Debug\n",
        "    # print(f'input_ids is on: {inputs[\"input_ids\"].device}')  # Debug\n",
        "\n",
        "    output = model.generate(**inputs,\n",
        "                            max_new_tokens=200,\n",
        "                            # do_sample=False,\n",
        "                            pad_token_id=tokenizer.pad_token_id,\n",
        "                            eos_token_id=tokenizer.eos_token_id,\n",
        "                            # temperature=0.01,\n",
        "                            # top_k=0\n",
        "                           )\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Subtract the length of input_ids from output to get only the model response\n",
        "    output_text = tokenizer.decode(output[0, len(inputs.input_ids[0]):], \\\n",
        "                                   skip_special_tokens=False)\n",
        "    output_text = re.sub('\\n+', '\\n', output_text)  # remove excessive newlines\n",
        "\n",
        "    print(\"**Generated Assistant Response:**\")\n",
        "    print(output_text)\n",
        "\n",
        "    print()\n",
        "\n",
        "    print(\"**Correct Assistant Response:**\")\n",
        "    print(correct_answer)\n",
        "\n",
        "    print()\n",
        "\n",
        "    # Clear GPU cache and run garbage collection\n",
        "    torch.cuda.empty_cache()  # Clear GPU cache\n",
        "    gc.collect()  # Run garbage collection"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run validation before fine-tuning\n",
        "Before fine-tuning the model, let's take a look at how the model responds to the validation set prompts.  \n",
        "\n",
        "Notice that the model should respond with a function name and query params, yet it tries writing code itself and includes lots of extra words."
      ],
      "metadata": {
        "id": "MBOpwW8hBKDo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pRFK3thQ1XV",
        "outputId": "5db2057c-7315-4383-cf06-732876e9a4d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Running index 0---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the names of the five largest stocks by market cap [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "To get the names of the five largest stocks by market cap, we can use the `get_big_stocks` function with the following parameters:\n",
            "* `number`: 5 (to get the names of the five largest stocks)\n",
            "* `region`: \"World\" (to consider all stocks in the world)\n",
            "Here's the code to do that:\n",
            "```python\n",
            "import requests\n",
            "def get_big_stocks(number, region):\n",
            "    url = f\"https://api.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={region}&apikey=YOUR_API_KEY\"\n",
            "    response = requests.get(url)\n",
            "    data = response.json()\n",
            "    market_cap = {}\n",
            "    for symbol in data[\"Time Series (Daily)\"].keys():\n",
            "        market_cap[symbol] = data[\"Time Series (\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": \"5\"\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 1---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the names of the five largest stocks in the US by market cap [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "Here's the code to get the names of the five largest stocks in the US by market cap:\n",
            "```python\n",
            "import requests\n",
            "def get_big_stocks(params):\n",
            "    url = \"https://api.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&apikey=YOUR_API_KEY\".format(params[\"names\"][0])\n",
            "    response = requests.get(url)\n",
            "    data = response.json()\n",
            "    return data[\"Time Series (Daily)\"]\n",
            "def get_stock_price(params):\n",
            "    url = \"https://api.alphavantage.co/query?function=GLOBAL_QUOTE&symbol={}&apikey=YOUR_API_KEY\".format(params[\"names\"][0])\n",
            "    response = requests.get(url)\n",
            "    data = response\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": \"5\",\n",
            "        \"region\": \"US\"\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 2---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the stock prices of the ten largest stocks in the world [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "To get the stock prices of the ten largest stocks in the world, you can use the following steps:\n",
            "1. Get the names of the ten largest stocks in the world using the `get_big_stocks` function.\n",
            "2. Use the `get_stock_price` function to get the stock prices of the names obtained in step 1.\n",
            "Here's the code to implement this:\n",
            "```python\n",
            "import requests\n",
            "def get_big_stocks(params):\n",
            "    url = \"https://api.alphavantage.co/query?function=TIME_SERIES_DAILY_ADJUSTED&symbol={}&apikey=YOUR_API_KEY\".format(params[\"names\"][0])\n",
            "    response = requests.get(url)\n",
            "    data = response.json()\n",
            "    return data[\"Time Series (Daily)\"]\n",
            "def get_stock_price(\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": \"10\",\n",
            "        \"region\": \"World\"\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 3---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the price of Apple's stock [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "To get the price of Apple's stock, you can use the `get_stock_price` function. Here's an example of how you can use it:\n",
            "```\n",
            "const stockNames = [\"AAPL\"];\n",
            "const stockPrices = get_stock_price({ names: stockNames });\n",
            "console.log(stockPrices);\n",
            "```\n",
            "This will output the current price of Apple's stock. Note that you will need to have an API key to use this function.</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_stock_price\",\n",
            "    \"arguments\": {\n",
            "        \"names\": [\n",
            "            \"Apple\"\n",
            "        ]\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 4---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Greetings! [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "Hello! How can I assist you today?</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "Greetings to you too!\n",
            "\n",
            "---Running index 5---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "shop [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "To get the names of the largest 25 stocks by market cap in the US region, you can use the following code:\n",
            "```\n",
            "const bigStocks = await get_big_stocks({\n",
            "    number: 25,\n",
            "    region: \"US\"\n",
            "});\n",
            "console.log(bigStocks);\n",
            "```\n",
            "To get the stock price of an array of stocks, you can use the following code:\n",
            "```\n",
            "const stockPrices = await get_stock_price({\n",
            "    names: [\"AAPL\", \"GOOG\", \"AMZN\"]\n",
            "});\n",
            "console.log(stockPrices);\n",
            "```\n",
            "Note that the `get_stock_price` function requires an array of stock names as input, and returns an object with the stock prices as keys and their corresponding values.</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "What would you like to know about shops?\n",
            "\n",
            "---Running index 6---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "What are the planets in our solar system? [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "I'm sorry, but the provided functions do not seem to be related to the question about the planets in our solar system. Can you please provide more context or clarify your question?</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run validation before training\n",
        "for index in range(len(test_dataset)):\n",
        "    print(f'---Running index {index}---')\n",
        "    generate(index, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J53zPp1qSdZp"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "VSjTZHnSUFEU"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LMiT3lsLz-zc"
      },
      "outputs": [],
      "source": [
        "class CustomTrainer(transformers.Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Define number of tokens to display\n",
        "        # Displays actual and predicted token info at end of each sequence\n",
        "        num_tokens = 25\n",
        "\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        # # Get first hundred label IDs for each sequence in the batch\n",
        "        # first_hundred_label_ids = labels[:, :200]\n",
        "\n",
        "        # # Convert to tokens\n",
        "        # first_hundred_tokens = [tokenizer.convert_ids_to_tokens(label_ids) \\\n",
        "        # for label_ids in first_hundred_label_ids]\n",
        "\n",
        "        # # Print them\n",
        "        # for batch_idx, tokens in enumerate(first_hundred_tokens):\n",
        "        #     print(f\"First 200 decoded tokens for sequence {batch_idx + 1}: {tokens}\")\n",
        "\n",
        "        loss_mask = inputs.pop(\"loss_mask\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Check for NaN in logits and labels\n",
        "        if torch.isnan(logits).any():\n",
        "            print(\"NaN detected in logits\")\n",
        "            print(logits)\n",
        "\n",
        "        # Convert logits to probabilities using softmax function\n",
        "        probs = nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # Get the most probable tokens\n",
        "        predicted_token_ids = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "        losses = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        # Reshaping the losses to have dimensions [batch_size, seq_length]\n",
        "        losses = losses.view(-1, inputs['input_ids'].size(1))\n",
        "\n",
        "        # Apply the loss mask\n",
        "        masked_loss = losses * loss_mask\n",
        "\n",
        "        # Check for NaN in losses and zero in loss_mask.sum()\n",
        "        if torch.isnan(losses).any():\n",
        "            print(\"NaN detected in losses\")\n",
        "            # print(losses)\n",
        "\n",
        "        if loss_mask.sum() == 0:\n",
        "            print(\"Sum of loss_mask is zero\")\n",
        "            return (torch.tensor(0).to(loss_mask.device), outputs) \\\n",
        "            if return_outputs else torch.tensor(0).to(loss_mask.device)  # Early return\n",
        "\n",
        "        # Aggregate the masked losses\n",
        "        # Normalize by the number of tokens considered + epsilon to prevent\n",
        "        # division by zero\n",
        "        loss = masked_loss.sum() / (loss_mask.sum() + 1e-9)\n",
        "\n",
        "        # Print formatted tokens\n",
        "        batch_size, seq_length = inputs['input_ids'].size()\n",
        "\n",
        "        # num_tokens = len(inputs['input_ids'][0])\n",
        "\n",
        "        # # Useful for debugging training\n",
        "        # # Recommend training a small number of steps\n",
        "        # print(\"-\" * 120)\n",
        "        # print(f\"Token analysis for last {num_tokens} tokens:\")\n",
        "        # header_format = \"{:<10}{:<20}{:<20}{:<20}{:<20}{:<30}{:<30}\".format(\"Index\", \"Input Token\", \"Predicted Token\", \"True Token\", \"Loss Mask\", \"Raw Loss\", \"Masked Loss\")\n",
        "\n",
        "        # for batch_idx in range(batch_size):\n",
        "        #     input_tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][batch_idx])  # Using batch_idx\n",
        "        #     predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_token_ids[batch_idx])  # Using batch_idx\n",
        "        #     true_tokens = tokenizer.convert_ids_to_tokens(labels[batch_idx])  # Using batch_idx\n",
        "\n",
        "        #     print(f\"\\nBatch {batch_idx + 1} of {batch_size}:\")\n",
        "        #     print(header_format)\n",
        "        #     for i in range(-num_tokens, 0, 1):\n",
        "        #         index = seq_length + i  # Correct index based on sequence length\n",
        "        #         print(\"{:<10}{:<20}{:<20}{:<20}{:<20.1f}{:<30.6f}{:<30.6f}\".format(index, input_tokens[index], predicted_tokens[index], true_tokens[index], loss_mask[batch_idx, i].item(), losses[batch_idx, i], masked_loss[batch_idx, i]))\n",
        "        #     print(\"-\" * 120)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "      train_dataset = self.train_dataset\n",
        "      data_collator = self.data_collator\n",
        "\n",
        "      dataloader_params = {\n",
        "          \"batch_size\": self.args.train_batch_size,\n",
        "          \"collate_fn\": data_collator,\n",
        "          \"num_workers\": self.args.dataloader_num_workers,\n",
        "          \"pin_memory\": self.args.dataloader_pin_memory,\n",
        "      }\n",
        "\n",
        "      if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
        "          dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
        "          dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
        "\n",
        "      return DataLoader(train_dataset, **dataloader_params)\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset=None):\n",
        "      eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "      if eval_dataset is None:\n",
        "          raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
        "\n",
        "      data_collator = self.data_collator\n",
        "\n",
        "      # Parameters for the DataLoader\n",
        "      dataloader_params = {\n",
        "          \"batch_size\": self.args.eval_batch_size,\n",
        "          \"collate_fn\": data_collator,\n",
        "          \"num_workers\": self.args.dataloader_num_workers,\n",
        "          \"pin_memory\": self.args.dataloader_pin_memory,\n",
        "      }\n",
        "\n",
        "      # If your dataset isn't an instance of torch's IterableDataset, you can\n",
        "      # provide sampler and drop_last\n",
        "      if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
        "          dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
        "          # Typically we don't drop the last batch for evaluation\n",
        "          dataloader_params[\"drop_last\"] = False\n",
        "\n",
        "      return DataLoader(eval_dataset, **dataloader_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "lz9auc1jCvBg"
      },
      "outputs": [],
      "source": [
        "class CustomDataCollator: # Needed if the EOS token is included in training\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "\n",
        "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "        labels = torch.stack([item['labels'] for item in batch])\n",
        "        loss_mask = torch.stack([item['loss_mask'] for item in batch])\n",
        "\n",
        "        # # Debugging: print details of the first sequence in the batch\n",
        "        # num_elements_to_view = 20  # Number of last elements to view\n",
        "\n",
        "        # # Decoding the input_ids\n",
        "        # decoded_input_tokens = self.tokenizer.convert_ids_to_tokens(input_ids[0].tolist())\n",
        "\n",
        "        # print(\"Debugging last\", num_elements_to_view, \"elements of the first sequence in the batch:\")\n",
        "        # print(\"{:<20}{:<20}{:<20}{:<20}\".format(\"Token\", \"Input ID\", \"Label\", \"Loss Mask\"))\n",
        "        # for i in range(-num_elements_to_view, 0, 1):\n",
        "        #   print(\"{:<20}{:<20}{:<20}{:<20}\".format(decoded_input_tokens[i], input_ids[0, i].item(), labels[0, i].item(), loss_mask[0, i].item()))\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels,\n",
        "            'loss_mask': loss_mask\n",
        "        }\n",
        "\n",
        "data_collator = CustomDataCollator(tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "iDniJ-bq4nxD"
      },
      "outputs": [],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=validation_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        # max_steps=1,\n",
        "        num_train_epochs=1, # Larger models typically only need 1 epoch\n",
        "        per_device_train_batch_size=1,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        max_grad_norm=1,\n",
        "        warmup_ratio=0.1,\n",
        "        eval_steps=0.2,\n",
        "        learning_rate=1e-4, # 1e-4 for LoRA\n",
        "        # learning_rate=1e-5, # 1e-5 for full fine-tuning\n",
        "        # fp16=True, # If not using an Ampere series (i.e. not H100, A100, A6000)\n",
        "        bf16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        # optim=\"paged_adamw_8bit\", # For training in 4bit (quantized)\n",
        "        optim=\"adamw_torch\", # For training in full fp16/bf16 precision\n",
        "        lr_scheduler_type='constant',\n",
        "        hub_private_repo=True\n",
        "    ),\n",
        "    data_collator=data_collator,\n",
        "    # data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False  # Silence warnings (Set to True for inference!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "LsjbF5SZ6ifV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "d0a8ec14-b2e3-45f2-e4bf-624cd17fd08b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240206_060238-j0g979ti</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gadkins/huggingface/runs/j0g979ti' target=\"_blank\">still-capybara-2</a></strong> to <a href='https://wandb.ai/gadkins/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gadkins/huggingface' target=\"_blank\">https://wandb.ai/gadkins/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gadkins/huggingface/runs/j0g979ti' target=\"_blank\">https://wandb.ai/gadkins/huggingface/runs/j0g979ti</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [66/66 00:29, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.109000</td>\n",
              "      <td>0.818482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.030600</td>\n",
              "      <td>0.809609</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.767155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.151700</td>\n",
              "      <td>0.759370</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "trainer.train()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWXT-FNfSlpk"
      },
      "source": [
        "# Example After Fine Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "_9vrd1FXSz8W",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e9c1315-247f-40f9-dc02-e7ccd6e8107a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MistralForCausalLM(\n",
              "      (model): MistralModel(\n",
              "        (embed_tokens): Embedding(32000, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x MistralDecoderLayer(\n",
              "            (self_attn): MistralAttention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): MistralRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): MistralMLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=4096, out_features=14336, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=14336, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=14336, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=14336, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): MistralRMSNorm()\n",
              "            (post_attention_layernorm): MistralRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): MistralRMSNorm()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "model.config.use_cache = True\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MezO1msTSnEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c6a35ef-b6d3-4aa8-9c34-0cfcd79b52de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---Running index 0---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the names of the five largest stocks by market cap [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": 5\n",
            "    }\n",
            "}</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": \"5\"\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 1---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the names of the five largest stocks in the US by market cap [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": 5\n",
            "    }\n",
            "}</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": \"5\",\n",
            "        \"region\": \"US\"\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 2---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the stock prices of the ten largest stocks in the world [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": 10\n",
            "    }\n",
            "}</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": \"10\",\n",
            "        \"region\": \"World\"\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 3---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Get the price of Apple's stock [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_stock_price\",\n",
            "    \"arguments\": {\n",
            "        \"names\": [\n",
            "            \"AAPL\"\n",
            "        ]\n",
            "    }\n",
            "}</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_stock_price\",\n",
            "    \"arguments\": {\n",
            "        \"names\": [\n",
            "            \"Apple\"\n",
            "        ]\n",
            "    }\n",
            "}\n",
            "\n",
            "---Running index 4---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "Greetings! [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "Hello! How can I assist you today?</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "Greetings to you too!\n",
            "\n",
            "---Running index 5---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "shop [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "{\n",
            "    \"name\": \"get_big_stocks\",\n",
            "    \"arguments\": {\n",
            "        \"number\": 25\n",
            "    }\n",
            "}</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "What would you like to know about shops?\n",
            "\n",
            "---Running index 6---\n",
            "Using the test data split.\n",
            "\n",
            "Prompt:\n",
            "[INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_stock_price\",\n",
            "            \"description\": \"Get the stock price of an array of stocks\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"names\": {\n",
            "                        \"type\": \"array\",\n",
            "                        \"items\": {\n",
            "                            \"type\": \"string\"\n",
            "                        },\n",
            "                        \"description\": \"An array of stocks\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"names\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_big_stocks\",\n",
            "            \"description\": \"Get the names of the largest N stocks by market cap\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"number\": {\n",
            "                        \"type\": \"integer\",\n",
            "                        \"description\": \"The number of largest stocks to get the names of, e.g. 25\"\n",
            "                    },\n",
            "                    \"region\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The region to consider, can be \\\"US\\\" or \\\"World\\\".\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"number\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]    \n",
            "\n",
            "What are the planets in our solar system? [/INST]\n",
            "\n",
            "\n",
            "\n",
            "**Generated Assistant Response:**\n",
            "The planets in our solar system are: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, and Neptune</s>\n",
            "\n",
            "**Correct Assistant Response:**\n",
            "The planets are Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus and Neptune.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Run validation\n",
        "for index in range(len(test_dataset)):\n",
        "    print(f'---Running index {index}---')\n",
        "    generate(index, \"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6FqNGBAz7ct"
      },
      "source": [
        "# Merge Adapters and Save Model to Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VVF0R6ZhBlaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b75953a-d97f-42ad-f1f6-4257ffeb11a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adapter Model: gadkins/Mistral-7B-Instruct-v0.1-function-calling-adapters\n",
            "New Model: gadkins/Mistral-7B-Instruct-v0.1-function-calling\n"
          ]
        }
      ],
      "source": [
        "# Extract the last portion of the base_model\n",
        "base_model_name = base_model.split(\"/\")[-1]\n",
        "\n",
        "adapter_model = f\"gadkins/{base_model_name}-function-calling-adapters\"\n",
        "new_model = f\"gadkins/{base_model_name}-function-calling\" # Your HF account\n",
        "\n",
        "print(f\"Adapter Model: {adapter_model}\\nNew Model: {new_model}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "pf3Vx9PDz9Q7"
      },
      "outputs": [],
      "source": [
        "# (Optional) Create repo + branch for gguf and awq\n",
        "\n",
        "from huggingface_hub import HfApi, create_branch, create_repo\n",
        "\n",
        "# Initialize the HfApi class\n",
        "api = HfApi()\n",
        "\n",
        "create_repo(new_model, private=False)\n",
        "\n",
        "create_branch(new_model, repo_type=\"model\", branch=\"gguf\")\n",
        "\n",
        "# create_branch(new_model, repo_type=\"model\", branch=\"awq\")\n",
        "\n",
        "# create_branch(new_model, repo_type=\"model\", branch=\"gptq\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "Wa8cQmW1z9Q7"
      },
      "outputs": [],
      "source": [
        "# model.config._name_or_path=\"gadkins/Yi-34B-200K-Llamafied-chat-SFT\"\n",
        "# print(model.config._name_or_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "e19FlnnCBlrM"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "model.save_pretrained(adapter_model, push_to_hub=True, use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "XiNiwQgfz9Q8"
      },
      "outputs": [],
      "source": [
        "# Push the model to the hub\n",
        "# model.push_to_hub(adapter_model, use_auth_token=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "zZPgnENAHmuN"
      },
      "outputs": [],
      "source": [
        "# # ## reload the base model (you might need a pro subscription for this because you may need a high RAM environment since this is loading the full original model, not quantized)\n",
        "# # ## you may prefer to use auto instead of cpu if you have a gpu\n",
        "# # ## if you are training in full precision (not quantized), you may not need to reload the model, you can directly merge and unload.\n",
        "# # ## if you are training very large models you may need to restart the kernel and reload the base model as there may not be enough space on gpu.\n",
        "\n",
        "# # from transformers import AutoModelForCausalLM, PretrainedConfig\n",
        "# # import torch\n",
        "\n",
        "# # model = AutoModelForCausalLM.from_pretrained(base_model, device_map='auto', trust_remote_code=True, torch_dtype=torch.float16, cache_dir=cache_dir)\n",
        "\n",
        "# from peft import PeftModel\n",
        "\n",
        "# # load perf model with new adapters\n",
        "# model = PeftModel.from_pretrained(\n",
        "#     model,\n",
        "#     './gadkins/Yi-34B-200K-Llamafied-chat-SFT-function-calling-adapters-v2',\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "YOQybEtGHJ9v"
      },
      "outputs": [],
      "source": [
        "model = model.merge_and_unload() # merge adapters with the base model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "e4RgrW13z9Q8"
      },
      "outputs": [],
      "source": [
        "# (Optional) Allows you to save the model locally to do inference without downloading\n",
        "model.save_pretrained(f\"gadkins/{base_model_name}-function-calling-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "vTKqNQQpIxL6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196,
          "referenced_widgets": [
            "9719388189954d3695f0e71827061de7",
            "3e28106a23084e7692b7e4fc3c687592",
            "c05ca773a2184b3aa828901d73bd63d3",
            "a3694fa6f4164b98bf363be05db4a669",
            "99e17646332042f3b20bb0a85d211f5c",
            "b6cb1d804ee547f9a3e31c7147772378",
            "5ba6faa7e5b349afa4f6fe765f18712b",
            "c7e0ef3e52064bbd980419ecea45d92e",
            "1ef7d24b84714aa4ba98ad0a25d89309",
            "4d5df9f6d2634fc2858a886f5456bbd5",
            "8dddc279b7b0465aa86822982a1e4606",
            "c3f6cf47fbb94263840e0710bd6c69fe",
            "80373584d64643699c6ceb20c5cc26e4",
            "e90a4faaf8224e6680ee914f80022ede",
            "1f61ad3321eb4b469585851ff7606722",
            "cbc710dc92e9445d89a0a8141a777580",
            "77e79f8d0ad746f4b5fe0ab2bc746a04",
            "fdde9d2c31d140edbb142d9e58fec359",
            "a3324e23fbf547b59d8858a3fcb3786c",
            "82cd30e1fa1c46f4b44725ccc13662bc",
            "8518a487ec124902966b8cf2e19e27ef",
            "90cf8cbb099d47a3bfd56f6bfc5d50e7",
            "b8663990dd2340a982faab7caa3758af",
            "e1090cb610434fe9b31ae3f0e7055d1c",
            "9a1db43f46154f21b464579a20ea744a",
            "2233ad452ab0478bbaab003a4276dfc1",
            "8a608bd6f324441d9e78dce56a7eadc0",
            "63b78d06585f4a9facda6dbeeaa13a58",
            "a17a71cd38764106965b072b2aa35d8d",
            "90fa1674dba14db5a79ea87d8fe876eb",
            "ed8cc464802b46938e97c675adf65acb",
            "64be22cb270b4f6799b2874aa5d31819",
            "3c96823b54bd4e10b178e55cd3748ca8",
            "4c40e3b89f274fb294c3efd7cf14b566",
            "1a15f64da1374104a3678fa53fa22cd8",
            "ffadb89d2f7e4e5792343faeedcf4ffd",
            "bd36a146065e4851bbd238b41e0ba059",
            "4c40ffac135e4cdebd265c337abcc00e",
            "8963c52752d241639c6c59c850a3aec5",
            "a5edd576f69f440c835594f7a8351e93",
            "c9695435f1374804ae19ad159fdb16a4",
            "c96e8dae6fe74c13b07b4996f3780257",
            "c39614be508445d0a60f19f06fb8ef03",
            "0ea0dfd9923f4fadb734b9efddf33bb7"
          ]
        },
        "outputId": "260ca143-e83d-455c-e3d8-fdc5c7db3fc0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/5.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9719388189954d3695f0e71827061de7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c3f6cf47fbb94263840e0710bd6c69fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8663990dd2340a982faab7caa3758af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/9.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c40e3b89f274fb294c3efd7cf14b566"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/gadkins/Mistral-7B-Instruct-v0.1-function-calling/commit/ca16aa19012184d3f5721a3a4ba7829876a385d1', commit_message='Upload MistralForCausalLM', commit_description='', oid='ca16aa19012184d3f5721a3a4ba7829876a385d1', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 81
        }
      ],
      "source": [
        "model.push_to_hub(new_model, token=True, max_shard_size=\"10GB\",safe_serialization=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJVv1Qkzz9Q9"
      },
      "source": [
        "### Base README.md and also tokenizer.model (needed for GGUF and GPTQ)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "DAs2ctluz9Q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7b0b450-172d-4836-f250-a9447a3ce586"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully downloaded tokenizer.model\n",
            "Uploaded tokenizer.model to gadkins/Mistral-7B-Instruct-v0.1-function-calling\n",
            "Successfully downloaded README.md\n",
            "Uploaded README.md to gadkins/Mistral-7B-Instruct-v0.1-function-calling\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "def download_file_from_huggingface(model_id, filename, save_path):\n",
        "    url = f\"https://huggingface.co/{model_id}/resolve/main/{filename}\"\n",
        "    r = requests.get(url)\n",
        "    if r.status_code != 200:\n",
        "        print(f\"Failed to download {filename}. HTTP Status Code: {r.status_code}\")\n",
        "        return False\n",
        "    with open(os.path.join(save_path, filename), 'wb') as f:\n",
        "        f.write(r.content)\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    # Files to download and upload\n",
        "    files_to_process = [\"tokenizer.model\", \"README.md\"]\n",
        "\n",
        "    # Directory to save the downloaded files\n",
        "    save_path = \"./models\"\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "\n",
        "    # Initialize HfApi class\n",
        "    api = HfApi()\n",
        "\n",
        "    # Specify the repository where you want to upload the files\n",
        "    repo_id = new_model  # Assuming new_model is in the format \"username/repo\"\n",
        "\n",
        "    for filename in files_to_process:\n",
        "        # Download the file\n",
        "        success = download_file_from_huggingface(base_model, filename, save_path)\n",
        "        if success:\n",
        "            print(f\"Successfully downloaded {filename}\")\n",
        "        else:\n",
        "            print(f\"Failed to download {filename}\")\n",
        "            continue  # Skip uploading if download failed\n",
        "\n",
        "        # File path to upload\n",
        "        local_file_path = os.path.join(save_path, filename)\n",
        "\n",
        "        # Upload the file\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=local_file_path,\n",
        "            path_in_repo=filename,  # Using filename directly, adjust as needed\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\",  # Assuming it's a model; can be \"dataset\" or \"space\" as well\n",
        "        )\n",
        "        print(f\"Uploaded {filename} to {repo_id}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vN8qKn7oz9Q9"
      },
      "source": [
        "## Set up chat template (advanced option)\n",
        "This is a more advanced step that allows you to customize a chat template for function calling.\n",
        "\n",
        "Typically you need to start by grabbing the `chat_template` from `tokenizer_config.json` of the base file and pasting that into the box below. You then need to customize that template to include `function_metadata`, `function_response` and `function_call` roles. You can see one example below but it won't be correct for all models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "48zwcWZCz9Q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6e1492f-0575-4b18-a406-62913ce7e3e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token + ' ' }}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "DHfTchsdz9Q9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dfc2438-9cd0-4871-ba63-155334681e09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s>\n",
            "</s>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.bos_token)\n",
        "print(tokenizer.eos_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "XQkXOhIMz9Q9"
      },
      "outputs": [],
      "source": [
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "7nNn6s0Iz9Q-"
      },
      "outputs": [],
      "source": [
        "function_metadata = [\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_current_weather\",\n",
        "            \"description\": \"This function gets the current weather in a given city\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"city\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The city, e.g., San Francisco\"\n",
        "                    },\n",
        "                    \"format\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                        \"description\": \"The temperature unit to use.\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"city\"]\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        \"type\": \"function\",\n",
        "        \"function\": {\n",
        "            \"name\": \"get_clothes\",\n",
        "            \"description\": \"This function provides a suggestion of clothes to wear based on the current weather\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"temperature\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The temperature, e.g., 15 C or 59 F\"\n",
        "                    },\n",
        "                    \"condition\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"The weather condition, e.g., 'Cloudy', 'Sunny', 'Rainy'\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"temperature\", \"condition\"]\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "VMJD2co0z9Q-"
      },
      "outputs": [],
      "source": [
        "# Comment out later messages to test various stages of generation.\n",
        "\n",
        "sample_messages = [\n",
        "    # {\n",
        "    #     \"role\": \"system\",\n",
        "    #     \"content\": \"you are a helpful assistant\"\n",
        "    # },\n",
        "    {\n",
        "        \"role\": \"function_metadata\",\n",
        "        \"content\": \"FUNCTION_METADATA\"\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"What is the current weather in London?\"\n",
        "    },\n",
        "    # {\n",
        "    #     \"role\": \"function_call\",\n",
        "    #     \"content\": \"{\\n    \\\"name\\\": \\\"get_current_weather\\\",\\n    \\\"arguments\\\": {\\n        \\\"city\\\": \\\"London\\\"\\n    }\\n}</s>\"\n",
        "    # },\n",
        "    # {\n",
        "    #     \"role\": \"function_response\",\n",
        "    #     \"content\": \"{\\n    \\\"temperature\\\": \\\"15 C\\\",\\n    \\\"condition\\\": \\\"Cloudy\\\"\\n}\"\n",
        "    # },\n",
        "    # {\n",
        "    #     \"role\": \"assistant\",\n",
        "    #     \"content\": \"The current weather in London is Cloudy with a temperature of 15 Celsius.</s>\"\n",
        "    # },\n",
        "    # {\n",
        "    #     \"role\": \"user\",\n",
        "    #     \"content\": \"That's great. Now say hello.\"\n",
        "    # },\n",
        "    # {\n",
        "    #     \"role\": \"assistant\",\n",
        "    #     \"content\": \"Hello!</s>\"\n",
        "    # }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "7aiKI5i8z9Q-"
      },
      "outputs": [],
      "source": [
        "# Iterate through each message in the list\n",
        "for message in sample_messages:\n",
        "    if message['role'] == 'function_metadata':\n",
        "        # Replace 'FUNCTION_METADATA' with 'function_metadata' in the content\n",
        "        message['content'] = message['content'].replace('FUNCTION_METADATA', json.dumps(function_metadata, indent=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "SV9rIZx8z9Q-"
      },
      "outputs": [],
      "source": [
        "# Llama 2 templates / Mistral\n",
        "tokenizer.chat_template = \"\"\"{{ bos_token }} [INST] {% for message in messages %}{% if message['role'] == 'system' %}<<SYS>>\\n{{ message['content'] }}\\n<</SYS>>\\n\\n{% elif message['role'] == 'function_metadata' %}You have access to the following functions. Use them if required:\\n\\n{{ message['content'] }}\\n\\n{% elif message['role'] == 'user' %}{{ message['content'] }} [/INST]\\n\\n{% elif message['role'] == 'assistant' %}{{ message['content'] }} [INST] {% elif message['role'] == 'function_call' %}{{ message['content'] }} [INST] {% elif message['role'] == 'function_response' %}Here is the response to the function call. If helpful, use it to respond to my question:\\n\\n{{ message['content'] }} [/INST]\\n\\n{% endif %}{% endfor %}\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "eMKFH95vz9Q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a85e6ffb-671a-45d8-8b53-06e13bcd7fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{{ bos_token }} [INST] {% for message in messages %}{% if message['role'] == 'system' %}<<SYS>>\n",
            "{{ message['content'] }}\n",
            "<</SYS>>\n",
            "\n",
            "{% elif message['role'] == 'function_metadata' %}You have access to the following functions. Use them if required:\n",
            "\n",
            "{{ message['content'] }}\n",
            "\n",
            "{% elif message['role'] == 'user' %}{{ message['content'] }} [/INST]\n",
            "\n",
            "{% elif message['role'] == 'assistant' %}{{ message['content'] }} [INST] {% elif message['role'] == 'function_call' %}{{ message['content'] }} [INST] {% elif message['role'] == 'function_response' %}Here is the response to the function call. If helpful, use it to respond to my question:\n",
            "\n",
            "{{ message['content'] }} [/INST]\n",
            "\n",
            "{% endif %}{% endfor %}\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.chat_template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "nAxUuFKlz9Q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7f95f6e-9600-41a0-c073-f2a7f28417a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<s> [INST] You have access to the following functions. Use them if required:\n",
            "\n",
            "[\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_current_weather\",\n",
            "            \"description\": \"This function gets the current weather in a given city\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"city\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The city, e.g., San Francisco\"\n",
            "                    },\n",
            "                    \"format\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"enum\": [\n",
            "                            \"celsius\",\n",
            "                            \"fahrenheit\"\n",
            "                        ],\n",
            "                        \"description\": \"The temperature unit to use.\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"city\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"type\": \"function\",\n",
            "        \"function\": {\n",
            "            \"name\": \"get_clothes\",\n",
            "            \"description\": \"This function provides a suggestion of clothes to wear based on the current weather\",\n",
            "            \"parameters\": {\n",
            "                \"type\": \"object\",\n",
            "                \"properties\": {\n",
            "                    \"temperature\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The temperature, e.g., 15 C or 59 F\"\n",
            "                    },\n",
            "                    \"condition\": {\n",
            "                        \"type\": \"string\",\n",
            "                        \"description\": \"The weather condition, e.g., 'Cloudy', 'Sunny', 'Rainy'\"\n",
            "                    }\n",
            "                },\n",
            "                \"required\": [\n",
            "                    \"temperature\",\n",
            "                    \"condition\"\n",
            "                ]\n",
            "            }\n",
            "        }\n",
            "    }\n",
            "]\n",
            "\n",
            "What is the current weather in London? [/INST]\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# View the template applied without tokenization\n",
        "prompt = tokenizer.apply_chat_template(sample_messages, tokenize=False)\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "RoBR31Caz9Q-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdcfdca1-f96e-42a3-e3f9-eb2239b35933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "{\n",
            "    \"name\": \"get_current_weather\",\n",
            "    \"arguments\": {\n",
            "        \"city\": \"London\"\n",
            "    }\n",
            "}</s>\n"
          ]
        }
      ],
      "source": [
        "## Test generation\n",
        "\n",
        "inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "if \"token_type_ids\" in inputs:\n",
        "    del inputs[\"token_type_ids\"]\n",
        "\n",
        "# print(f'model is on: {next(model.parameters()).device}')  # Debug line\n",
        "# print(f'input_ids is on: {inputs[\"input_ids\"].device}')  # Debug line\n",
        "\n",
        "output = model.generate(**inputs,\n",
        "                        max_new_tokens=200,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=tokenizer.pad_token_id,\n",
        "                        eos_token_id=tokenizer.eos_token_id,\n",
        "                        # temperature=0.01,\n",
        "                        # top_k=0\n",
        "                       )\n",
        "\n",
        "print()\n",
        "\n",
        "# Subtract the length of input_ids from output to get only the model's response\n",
        "output_text = tokenizer.decode(output[0, len(inputs.input_ids[0]):], skip_special_tokens=False)\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_g-csiL5z9Q_"
      },
      "source": [
        "## Push Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "-uT02OtYz9Q_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b71110-b663-468b-cb66-6c793bd2f7a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('gadkins/Mistral-7B-Instruct-v0.1-function-calling-v3/tokenizer_config.json',\n",
              " 'gadkins/Mistral-7B-Instruct-v0.1-function-calling-v3/special_tokens_map.json',\n",
              " 'gadkins/Mistral-7B-Instruct-v0.1-function-calling-v3/tokenizer.model',\n",
              " 'gadkins/Mistral-7B-Instruct-v0.1-function-calling-v3/added_tokens.json',\n",
              " 'gadkins/Mistral-7B-Instruct-v0.1-function-calling-v3/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "# optional, but allows you to save the model locally so you can immediately inference without downloading\n",
        "tokenizer.save_pretrained(f\"gadkins/{base_model_name}-function-calling-v3\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "buqIU-9VJxVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "c9aa5617d56b4918847fccd2a979fd7c",
            "d2a5b60c2a05466a9de4840922bdc5a0",
            "e746bef86b7a4e3dadb2a971b0d63128",
            "96d6b741253c457a809542044d260e7d",
            "a97b84e98c9d49fba66d9058247c57cc",
            "b9b7c9cabb0a4c68adbd149e8a0576af",
            "1c8100ff9d9e4c7b91838e191cf3f76f",
            "1e8019a13b5641fdbde8fc46e764003e",
            "c5fe87f8b5a4481cad154524c4f84bce",
            "082574115e6543aaa5e785074c09d820",
            "0ea2a49cde954e64904cbc61510de655"
          ]
        },
        "outputId": "6597456d-7133-4a2a-bb41-7f7a9c48bd54"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9aa5617d56b4918847fccd2a979fd7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/gadkins/Mistral-7B-Instruct-v0.1-function-calling/commit/dfe2015a6083826389d11212b55d530816a0e0c6', commit_message='Upload tokenizer', commit_description='', oid='dfe2015a6083826389d11212b55d530816a0e0c6', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "# #Push the tokenizer\n",
        "tokenizer.push_to_hub(new_model, token=True)\n",
        "\n",
        "## RELOAD IF NEEDED (NOT RECOMMENDED IF tokenizer.chat_template was updated.\n",
        "# from transformers import AutoTokenizer\n",
        "# # reload the tokenizer because you don't want to have an off-size tokenizer with pad tokens.\n",
        "# tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
